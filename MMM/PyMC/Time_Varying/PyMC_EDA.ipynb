{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "import isort\n",
    "isort.file(\"PyMC_EDA.ipynb\")\n",
    "import pylint\n",
    "# import bambi as bmb\n",
    "import category_encoders as ce\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import formulae\n",
    "import graphviz\n",
    "import kaleido\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import plotly.graph_objects as go\n",
    "import pytensor.tensor as pt\n",
    "import pymc as pm\n",
    "import arviz as az\n",
    "import plotly.graph_objects as go\n",
    "import seaborn as sns\n",
    "import shap\n",
    "import statsmodels.api\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# import waterfall_chart\n",
    "# from waterfall_ax import WaterfallChart\n",
    "# import plotly.graph_objects as go\n",
    "# import waterfall_chart\n",
    "import xarray\n",
    "from catboost import CatBoostRegressor, Pool\n",
    "from fitter import Fitter, get_common_distributions, get_distributions\n",
    "from IPython.display import display, Markdown, Math\n",
    "from math import sqrt\n",
    "from multiprocessing import Pool\n",
    "from pandas.plotting import autocorrelation_plot, lag_plot\n",
    "# from pandas_profiling import ProfileReport as pd_prof\n",
    "from pmdarima import acf\n",
    "from pmdarima.arima import ADFTest, auto_arima\n",
    "from pmdarima.arima.utils import ndiffs\n",
    "from scipy import signal\n",
    "from scipy.optimize import curve_fit\n",
    "from scipy.stats import linregress\n",
    "from sklearn import metrics, preprocessing\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from ydata_profiling import ProfileReport as ydata_prof"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "spend = pd.read_csv('C:/Users/norri/Desktop/native.csv', parse_dates=['DATE'])\n",
    "spend = spend.resample('W-Sun', on='DATE').sum()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-25T10:24:18.420648400Z",
     "start_time": "2023-07-25T10:24:18.392548900Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Bayesian Testing"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"None of [Index(['display_s', 'brands_brandes_s', 'brands_competitive_s',\\n       'products_category_S', 'products_competitive_s', 'products_auto',\\n       'revenue'],\\n      dtype='object')] are in the [columns]\"",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[17], line 4\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m#### 1. This section continues on to modeling and exploration of modeling techniques\u001B[39;00m\n\u001B[0;32m      2\u001B[0m final_vars \u001B[38;5;241m=\u001B[39m [\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdisplay_s\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mbrands_brandes_s\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mbrands_competitive_s\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mproducts_category_S\u001B[39m\u001B[38;5;124m'\u001B[39m,\n\u001B[0;32m      3\u001B[0m               \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mproducts_competitive_s\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mproducts_auto\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mrevenue\u001B[39m\u001B[38;5;124m'\u001B[39m]\n\u001B[1;32m----> 4\u001B[0m df_final \u001B[38;5;241m=\u001B[39m \u001B[43mmedia\u001B[49m\u001B[43m[\u001B[49m\u001B[43mfinal_vars\u001B[49m\u001B[43m]\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\frame.py:3767\u001B[0m, in \u001B[0;36mDataFrame.__getitem__\u001B[1;34m(self, key)\u001B[0m\n\u001B[0;32m   3765\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m is_iterator(key):\n\u001B[0;32m   3766\u001B[0m         key \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(key)\n\u001B[1;32m-> 3767\u001B[0m     indexer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcolumns\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_get_indexer_strict\u001B[49m\u001B[43m(\u001B[49m\u001B[43mkey\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mcolumns\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m[\u001B[38;5;241m1\u001B[39m]\n\u001B[0;32m   3769\u001B[0m \u001B[38;5;66;03m# take() does not accept boolean indexers\u001B[39;00m\n\u001B[0;32m   3770\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mgetattr\u001B[39m(indexer, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdtype\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m) \u001B[38;5;241m==\u001B[39m \u001B[38;5;28mbool\u001B[39m:\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:5876\u001B[0m, in \u001B[0;36mIndex._get_indexer_strict\u001B[1;34m(self, key, axis_name)\u001B[0m\n\u001B[0;32m   5873\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m   5874\u001B[0m     keyarr, indexer, new_indexer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reindex_non_unique(keyarr)\n\u001B[1;32m-> 5876\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_raise_if_missing\u001B[49m\u001B[43m(\u001B[49m\u001B[43mkeyarr\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mindexer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maxis_name\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   5878\u001B[0m keyarr \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtake(indexer)\n\u001B[0;32m   5879\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(key, Index):\n\u001B[0;32m   5880\u001B[0m     \u001B[38;5;66;03m# GH 42790 - Preserve name from an Index\u001B[39;00m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:5935\u001B[0m, in \u001B[0;36mIndex._raise_if_missing\u001B[1;34m(self, key, indexer, axis_name)\u001B[0m\n\u001B[0;32m   5933\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m use_interval_msg:\n\u001B[0;32m   5934\u001B[0m         key \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(key)\n\u001B[1;32m-> 5935\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNone of [\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mkey\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m] are in the [\u001B[39m\u001B[38;5;132;01m{\u001B[39;00maxis_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m]\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m   5937\u001B[0m not_found \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(ensure_index(key)[missing_mask\u001B[38;5;241m.\u001B[39mnonzero()[\u001B[38;5;241m0\u001B[39m]]\u001B[38;5;241m.\u001B[39munique())\n\u001B[0;32m   5938\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mnot_found\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m not in index\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "\u001B[1;31mKeyError\u001B[0m: \"None of [Index(['display_s', 'brands_brandes_s', 'brands_competitive_s',\\n       'products_category_S', 'products_competitive_s', 'products_auto',\\n       'revenue'],\\n      dtype='object')] are in the [columns]\""
     ]
    }
   ],
   "source": [
    "#### 1. This section continues on to modeling and exploration of modeling techniques\n",
    "final_vars = ['display_s', 'brands_brandes_s', 'brands_competitive_s', 'products_category_S',\n",
    "              'products_competitive_s', 'products_auto', 'revenue']\n",
    "df_final = media[final_vars]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-25T10:26:12.754914100Z",
     "start_time": "2023-07-25T10:26:12.542947200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fitting 5 distributions: 100%|██████████| 5/5 [00:00<00:00, 84.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CTV\n",
      "         sumsquare_error          aic          bic  kl_div  ks_statistic   \n",
      "norm        5.770272e-11  2962.670967 -2925.602412     inf      0.067726  \\\n",
      "gamma       5.796781e-11  2943.176074 -2920.481345     inf      0.056814   \n",
      "cauchy      6.180837e-11  2967.989963 -2918.454039     inf      0.084375   \n",
      "uniform     9.194246e-11  2828.548256 -2877.153121     inf      0.314663   \n",
      "expon       1.001151e-10  2898.034818 -2868.296720     inf      0.286556   \n",
      "\n",
      "            ks_pvalue  \n",
      "norm     7.008352e-01  \n",
      "gamma    8.711862e-01  \n",
      "cauchy   4.261883e-01  \n",
      "uniform  1.189394e-09  \n",
      "expon    4.713382e-08  \n",
      "{'norm': {'loc': 1139493.911927829, 'scale': 249573.48282987252}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fitting 5 distributions: 100%|██████████| 5/5 [00:00<00:00, 102.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DISPLAY\n",
      "         sumsquare_error          aic          bic  kl_div  ks_statistic   \n",
      "cauchy      4.474874e-12  3310.597543 -3191.512328     inf      0.110994  \\\n",
      "gamma       6.382564e-12  3177.267017 -3149.938411     inf      0.169478   \n",
      "norm        8.034658e-12  3186.677160 -3130.642573     inf      0.257274   \n",
      "expon       8.362653e-12  3170.442356 -3126.481394     inf      0.211167   \n",
      "uniform     9.399225e-12  3085.663340 -3114.328839     inf      0.427084   \n",
      "\n",
      "            ks_pvalue  \n",
      "cauchy   1.429266e-01  \n",
      "gamma    4.423332e-03  \n",
      "norm     1.472350e-06  \n",
      "expon    1.509883e-04  \n",
      "uniform  9.777252e-18  \n",
      "{'cauchy': {'loc': 6475239.948853964, 'scale': 356225.27396946633}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fitting 5 distributions: 100%|██████████| 5/5 [00:00<00:00, 113.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DIRECT_MAIL\n",
      "         sumsquare_error          aic          bic  kl_div  ks_statistic   \n",
      "gamma           0.000102  1423.240181 -1425.097563     inf      0.043561  \\\n",
      "cauchy          0.000120  1483.294098 -1412.368678     inf      0.123618   \n",
      "norm            0.000123  1442.437908 -1410.267693     inf      0.105744   \n",
      "uniform         0.000190  1334.806920 -1364.749974     inf      0.286204   \n",
      "expon           0.000200  1407.005884 -1359.447627     inf      0.231360   \n",
      "\n",
      "            ks_pvalue  \n",
      "gamma    9.842843e-01  \n",
      "cauchy   7.636964e-02  \n",
      "norm     1.817985e-01  \n",
      "uniform  4.924196e-08  \n",
      "expon    2.234118e-05  \n",
      "{'gamma': {'a': 4.256827626526109, 'loc': 179.46949469690907, 'scale': 77.2423531651113}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fitting 5 distributions: 100%|██████████| 5/5 [00:00<00:00, 60.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EMAIL\n",
      "         sumsquare_error          aic          bic  kl_div  ks_statistic   \n",
      "gamma       2.257987e-08  3015.935773 -2300.129032     inf      0.449620  \\\n",
      "expon       3.676226e-08  2759.844543 -2254.082442     inf      0.691181   \n",
      "norm        4.008973e-08  2682.160498 -2245.071028     inf      0.404154   \n",
      "uniform     4.017935e-08  2557.179683 -2244.838775     inf      0.698715   \n",
      "cauchy      4.021216e-08  4363.780329 -2244.753895     inf      0.305142   \n",
      "\n",
      "            ks_pvalue  \n",
      "gamma    1.050430e-19  \n",
      "expon    4.396345e-50  \n",
      "norm     7.403667e-16  \n",
      "uniform  2.218841e-51  \n",
      "cauchy   4.313459e-09  \n",
      "{'gamma': {'a': 0.21034598686376355, 'loc': 699949.032887245, 'scale': 93183.76310962439}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fitting 5 distributions: 100%|██████████| 5/5 [00:00<00:00, 63.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PAID_SOCIAL\n",
      "         sumsquare_error          aic          bic  kl_div  ks_statistic   \n",
      "gamma       5.593234e-11  3597.934825 -2924.198834     inf      0.411998  \\\n",
      "expon       9.182212e-11  3358.929422 -2877.289335     inf      0.688582   \n",
      "cauchy      9.394318e-11  4680.692859 -2874.914291     inf      0.296909   \n",
      "norm        1.001287e-10  3281.420960 -2868.282610     inf      0.403903   \n",
      "uniform     1.003524e-10  3156.421738 -2868.050512     inf      0.698286   \n",
      "\n",
      "            ks_pvalue  \n",
      "gamma    1.739480e-16  \n",
      "expon    1.213769e-49  \n",
      "cauchy   1.269196e-08  \n",
      "norm     7.750421e-16  \n",
      "uniform  2.635625e-51  \n",
      "{'gamma': {'a': 0.22955058213909485, 'loc': 13997587.128853159, 'scale': 1850537.4040267188}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fitting 5 distributions: 100%|██████████| 5/5 [00:00<00:00, 60.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TV\n",
      "         sumsquare_error          aic          bic  kl_div  ks_statistic   \n",
      "gamma       1.015401e-13  3521.867421 -3580.589062     inf      0.074325  \\\n",
      "norm        1.061936e-13  3512.995482 -3580.573180     inf      0.104036   \n",
      "cauchy      1.117994e-13  3568.919070 -3575.223208     inf      0.110857   \n",
      "uniform     1.280648e-13  3460.249860 -3561.096870     inf      0.156727   \n",
      "expon       1.634761e-13  3521.625696 -3535.707294     inf      0.305462   \n",
      "\n",
      "            ks_pvalue  \n",
      "gamma    5.875311e-01  \n",
      "norm     1.960940e-01  \n",
      "cauchy   1.438494e-01  \n",
      "uniform  1.069022e-02  \n",
      "expon    4.133426e-09  \n",
      "{'gamma': {'a': 15.933905234854107, 'loc': -7452579.190181471, 'scale': 1778405.4686704623}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fitting 5 distributions: 100%|██████████| 5/5 [00:00<00:00, 76.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REVENUE\n",
      "         sumsquare_error          aic          bic  kl_div  ks_statistic   \n",
      "cauchy      1.795185e-15  4066.258910 -4004.909354     inf      0.160445  \\\n",
      "gamma       1.895590e-15  3950.001855 -3994.605078     inf      0.114585   \n",
      "norm        2.510694e-15  3953.101029 -3970.022449     inf      0.179801   \n",
      "expon       2.566428e-15  3948.562934 -3967.739050     inf      0.210279   \n",
      "uniform     2.832110e-15  3870.549444 -3957.494314     inf      0.327383   \n",
      "\n",
      "            ks_pvalue  \n",
      "cauchy   8.325394e-03  \n",
      "gamma    1.204147e-01  \n",
      "norm     2.056653e-03  \n",
      "expon    1.635453e-04  \n",
      "uniform  1.988020e-10  \n",
      "{'cauchy': {'loc': 333052856.83703077, 'scale': 24390697.519964397}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[10], line 11\u001B[0m\n\u001B[0;32m      9\u001B[0m     \u001B[38;5;28mprint\u001B[39m(f\u001B[38;5;241m.\u001B[39msummary(plot\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m))\n\u001B[0;32m     10\u001B[0m     \u001B[38;5;28mprint\u001B[39m(f\u001B[38;5;241m.\u001B[39mget_best(method\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124msumsquare_error\u001B[39m\u001B[38;5;124m'\u001B[39m))\n\u001B[1;32m---> 11\u001B[0m df_final_cat \u001B[38;5;241m=\u001B[39m \u001B[43mdf\u001B[49m[final_vars_cat]\n\u001B[0;32m     12\u001B[0m df_final_cat \u001B[38;5;241m=\u001B[39m df[df_final_cat[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mTotal Sales\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0.0\u001B[39m]\n",
      "\u001B[1;31mNameError\u001B[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "dist_list = ['gamma', 'expon', 'cauchy', 'norm', 'uniform']\n",
    "\n",
    "for var in final_vars:\n",
    "    dist_test = df_final[var].dropna()\n",
    "    dist_test = dist_test.values\n",
    "    f = Fitter(dist_test, distributions=dist_list, timeout=60)\n",
    "    f.fit()\n",
    "    print(var)\n",
    "    print(f.summary(plot=False))\n",
    "    print(f.get_best(method='sumsquare_error'))\n",
    "df_final_cat = df[final_vars_cat]\n",
    "df_final_cat = df[df_final_cat['Total Sales'] > 0.0]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-25T10:06:47.498048300Z",
     "start_time": "2023-07-25T10:06:46.960307300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for var in final_vars:\n",
    "    dist_test = df_final[var].dropna()\n",
    "    dist_test = dist_test.values\n",
    "    f = Fitter(dist_test, distributions=dist_list, timeout=60)\n",
    "    f.fit()\n",
    "    print(var)\n",
    "    print(f.summary(plot=False))\n",
    "    print(f.get_best(method='sumsquare_error'))\n",
    "# The following two charts show that many Tactic Category observations have dropped and two categories have also dropped."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for col in df.select_dtypes(include='object'):\n",
    "    if df[col].nunique() <= 25:\n",
    "        sns.countplot(y=col, data=df)\n",
    "        plt.show()\n",
    "\n",
    "print('Original number of obs and Tactic Categories')\n",
    "print(df['Tactic Category'].count())\n",
    "print(df['Tactic Category'].nunique())\n",
    "for col in df_final_cat.select_dtypes(include='object'):\n",
    "    if df_final_cat[col].nunique() <= 25:\n",
    "        sns.countplot(y=col, data=df_final_cat)\n",
    "        plt.show()\n",
    "\n",
    "print('Number of obs, new tactics, and new displays')\n",
    "print(df_final_cat['Tactic Category'].count())\n",
    "print(df_final_cat['Tactic Category'].nunique())\n",
    "print(df_final_cat['Brand'].nunique())\n",
    "tt.config.floatX = 'float64'"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_final_cat = df[final_vars_cat]\n",
    "df_final_cat = df_final_cat[df_final_cat['Total Sales'] > 0.0]\n",
    "\n",
    "df_final_cat['Total Sales'] = np.log(df_final_cat['Total Sales'])\n",
    "\n",
    "price_decr_idxs, price_decr = pd.factorize(df_final_cat['Price Decr Only %ACV'],\n",
    "                                           sort=True)\n",
    "tactic_idxs, tactics = pd.factorize(df_final_cat['Tactic Category'], sort=True)\n",
    "brand_idxs, brand = pd.factorize(df_final_cat['Brand'], sort=True)\n",
    "coords = {\"tactics\": tactics, 'brand': brand, 'obs_idx': np.arange(len(tactic_idxs)),\n",
    "          'price_decrease': price_decr}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#### 3. Hierarchical Model with diagnostics\n",
    "####\n",
    "with pm.Model(coords=coords) as hierarchical_model:\n",
    "    tactic_idx = pm.Data(\"Tactic Category\", tactic_idxs, dims=\"obs_idx\")\n",
    "    brand_idx = pm.Data(\"Brand\", brand_idxs, dims=\"obs_idx\")\n",
    "    price_decr = pm.Data(\"Price Decrease Only\", price_decr_idxs, dims=\"obs_idx\")\n",
    "\n",
    "    # Tactic Category hyperpriors parameters:\n",
    "    hp_b_0 = pm.Normal(\"hp_b_0\", mu=0.0, sigma=5.0)\n",
    "    sigma_a = pm.Exponential(\"sigma_a\", 1.0)\n",
    "    hp_b_1 = pm.Normal(\"hp_b_1\", mu=0.0, sigma=1.0)\n",
    "    sigma_b = pm.Exponential(\"sigma_b\", 0.5)\n",
    "\n",
    "    # Brand hyperpriors parameters\n",
    "    hp_b_2 = pm.Normal(\"hp_b_2\", mu=0.0, sigma=5.0)\n",
    "    sigma_c = pm.Exponential(\"sigma_c\", 1.0)\n",
    "    hp_b_3 = pm.Normal(\"hp_b_3\", mu=0.0, sigma=1.0)\n",
    "    sigma_d = pm.Exponential(\"sigma_d\", 0.5)\n",
    "\n",
    "    hp_b_4 = pm.Normal(\"hp_b_4\", mu=0.5, sigma=.3)\n",
    "\n",
    "    # estimates of independent intercepts and interactions of intercepts\n",
    "    b_0 = pm.Normal(\"tactic_int\", mu=hp_b_0, sigma=sigma_a, dims=\"tactics\")\n",
    "    b_1 = pm.Normal(\"tactic_slope\", mu=hp_b_1, sigma=sigma_b, dims=\"tactics\")\n",
    "\n",
    "    b_2 = pm.Normal(\"display_int\", mu=hp_b_2, sigma=sigma_c, dims=\"brand\")\n",
    "    b_3 = pm.Normal(\"display_slope\", mu=hp_b_3, sigma=sigma_d, dims=\"brand\")\n",
    "\n",
    "    price_decr_est = hp_b_4 * price_decr\n",
    "\n",
    "    # estimate of total sales using intercepts\n",
    "    sales_est_1 = b_0[tactic_idx] + b_1[tactic_idx] * brand_idx\n",
    "    sales_est_2 = b_2[brand_idx] + b_3[brand_idx] * tactic_idx\n",
    "    sales_est = sales_est_1 + sales_est_2 + price_decr_est\n",
    "\n",
    "    # Data likelihood\n",
    "    epsilon = pm.Exponential(\"noise\", 1.0)\n",
    "    Total_Sales = pm.Normal(\n",
    "        \"Log_Total_Sales\", mu=sales_est, sigma=epsilon,\n",
    "        observed=df_final_cat['Total Sales'], dims=\"obs_idx\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "with hierarchical_model:\n",
    "    hierarchical_trace = pm.sample(draws=1500, init='advi+adapt_diag', chains=5,\n",
    "                                   tune=1500,\n",
    "                                   target_accept=0.8, return_inferencedata=True)\n",
    "####\n",
    "ppc = pm.sample_posterior_predictive(hierarchical_trace, samples=20481,\n",
    "                                     model=hierarchical_model)\n",
    "print('Model R-Squared')\n",
    "print(az.r2_score(df_final_cat['Total Sales'].values, ppc['Log_Total_Sales']))\n",
    "\n",
    "\n",
    "def score_model(trace, y, model_name):\n",
    "    \"\"\"\n",
    "\n",
    "    :param trace: model_trace\n",
    "    :param y: dependent data column\n",
    "    :param model_name: model itself\n",
    "    \"\"\"\n",
    "    ppc_mod = pm.sample_posterior_predictive(trace, samples=20841,\n",
    "                                             model=model_name)\n",
    "    pred = ppc_mod['Log_Total_Sales'].mean(axis=0)\n",
    "    mse = np.sqrt(mean_squared_error(y, pred))\n",
    "    print('The Mean Squared Error')\n",
    "    print(mse)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "score_model(hierarchical_trace, df_final_cat['Total Sales'], hierarchical_model)\n",
    "\n",
    "print('Trace Summary and Effective Sample Size')\n",
    "print(az.summary(hierarchical_trace, kind='stats'))\n",
    "print(az.summary(hierarchical_trace, kind='diagnostics'))\n",
    "az.plot_posterior(hierarchical_trace, hdi_prob=0.99)\n",
    "az.plot_energy(hierarchical_trace)\n",
    "plt.show()\n",
    "print('Bayesian fraction of missing information')\n",
    "print(az.bfmi(hierarchical_trace))\n",
    "az.plot_forest(hierarchical_trace, kind='ridgeplot')\n",
    "####\n",
    "pm.model_to_graphviz(hierarchical_model)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Time Series Lag and Autocorrelation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = pd.read_csv('G:/My Drive/IN/Data/Data_Standard/kind_forecast_train.csv',\n",
    "                 parse_dates=['ds'])\n",
    "df_test = pd.read_csv('G:/My Drive/IN/Data/Data_Standard/kind_forecast_test.csv',\n",
    "                      parse_dates=['ds'])\n",
    "dates = pd.read_csv('G:/My Drive/IN/Data/Forecast_Comparisons/dates.csv',\n",
    "                    parse_dates=['Date'])\n",
    "df = df.rename(columns={'ds': 'week', 'y': 'sales'})\n",
    "df_test = df_test.rename(columns={'ds': 'week', 'y': 'sales'})\n",
    "df = df.drop_duplicates(subset=['sales', 'week'])\n",
    "df['week'] = df['week'] = pd.to_datetime(df['week'])\n",
    "df = df.sort_values(by=['week'])\n",
    "df['sales'] = df['sales'].values\n",
    "df['index'] = df['week']\n",
    "df.set_index('index', inplace=True)\n",
    "df = df[['week','sales']]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# The following metrics are excellent in different situations; for example, RMSE is\n",
    "# excellent for comparing similar models.\n",
    "\n",
    "####\n",
    "def forecast_accuracy(forecast, Actuals):\n",
    "    mape = np.mean(np.abs(forecast - Actuals)/np.abs(Actuals))  # MAPE\n",
    "    mae = np.mean(np.abs(forecast - Actuals))    # MAE\n",
    "    mse = np.square(np.subtract(Actuals,forecast)).mean()\n",
    "    rmse = np.mean((forecast - Actuals)**2)**.5  # RMSE\n",
    "    smape = 1/len(Actuals) * np.sum(2 * np.abs(forecast-Actuals) / (np.abs(Actuals) + np.abs(forecast))*100)\n",
    "    return({'MAPE':mape, 'MSE':mse, 'MAE':mae, 'RMSE':rmse, 'SMAPE':smape})\n",
    "# There are three parameters of interest for an ARIMA model: d,p, and q. D refers to\n",
    "# differencing each previous value to make the model stationary. P is the term used\n",
    "# for how many lags can be used for prediction. Q is the order of moving average\n",
    "# to improve the model.\n",
    "####\n",
    "def plot_df(df, x, y, title=\"\", xlabel='Date', ylabel='Sales', dpi=100):\n",
    "    plt.figure(figsize=(12, 4), dpi=dpi)\n",
    "    plt.plot(x, y, color='blue')\n",
    "    plt.gca().set(title=title, xlabel=xlabel, ylabel=ylabel)\n",
    "    plt.show()\n",
    "plot_df(df, df['week'], df['sales'], title='Sales Over Time')\n",
    "# An increasing trend is clearly visible, though the sesaonality is not\n",
    "# quite as obvious, so we will test for it later.\n",
    "#     The Augmented Dickey-Fuller Test checks for the important\n",
    "#     condition of stationarity. This test has failed, meaning we have to\n",
    "# correct for non-stationarity.\n",
    "####\n",
    "adf_test = ADFTest(alpha = .05)\n",
    "adf_test.should_diff(df['sales'])\n",
    "#####\n",
    "train = df[:170]\n",
    "test = df[-40:]\n",
    "plt.plot(train)\n",
    "plt.plot(test)\n",
    "# Value = Base Level + Trend + Seasonality + Error - Additive Decomposition\n",
    "# Value = Base Level x Trend x Seasonality x Error - Multiplicative Decompisition\n",
    "#####\n",
    "multiplicative_decomposition = seasonal_decompose(df['sales'].values,\n",
    "                                                  model='multiplicative',\n",
    "                                                  period=52)\n",
    "additive_decomposition = seasonal_decompose(df['sales'].values, model='additive',\n",
    "                                            period=52)\n",
    "plt.rcParams.update({'figure.figsize': (20,14)})\n",
    "multiplicative_decomposition.plot().suptitle('Multiplicative Decomposition', fontsize=16)\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "additive_decomposition.plot().suptitle('Additive Decomposition', fontsize=16)\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "plt.show()\n",
    "# Both plots have a positive trend and display some seasonality, but the additive\n",
    "# seems to have significantly better reesiduals.\n",
    "####\n",
    "detrended = signal.detrend(df['sales'].values)\n",
    "plt.rcParams.update({'figure.figsize': (12,4)})\n",
    "plt.plot(detrended)\n",
    "plt.title('Sales Detrended', fontsize=16)\n",
    "# The plot no longer seems to be increasing in trend, hence we have detrended it.\n",
    "####\n",
    "result_mul = seasonal_decompose(df['sales'].values, model='multiplicative', period=52)\n",
    "deseasonalized = df['sales'].values / result_mul.seasonal\n",
    "plt.plot(deseasonalized)\n",
    "plt.title('Sales Deseasonalized', fontsize=16)\n",
    "plt.plot()\n",
    "# The plot does not show a strong removal of seasonality, so move on to some testing\n",
    "# The following plot does not have any drastic spikes that suggest strong seasonality.\n",
    "#####\n",
    "plt.rcParams.update({'figure.figsize':(10,4), 'figure.dpi':120})\n",
    "autocorrelation_plot(df['sales'].tolist())\n",
    "# Autocorrelation refers to correlation of a series to its own lags. Partial autocorrelation\n",
    "# refers to the correlation to a lag without reference to the lags between.\n",
    "#####\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 3), dpi=100)\n",
    "plot_acf(df['sales'].tolist(), lags=52, ax=axes[0])\n",
    "plot_pacf(df['sales'].tolist(), lags=52, ax=axes[1])\n",
    "# Along with the above plots, the following metrics will quantify each degree of\n",
    "# lag. Perfect autocorrelation is one, and positive means that the next value in\n",
    "# the series will likely be higher.\n",
    "####\n",
    "ac1 = df['sales'].autocorr(lag=1)\n",
    "print(\"One week Lag: \", ac1)\n",
    "ac2 = df['sales'].autocorr(lag=2)\n",
    "print(\"Two week Lag: \", ac2)\n",
    "ac3 = df['sales'].autocorr(lag=3)\n",
    "print(\"Three week Lag: \", ac3)\n",
    "ac4 = df['sales'].autocorr(lag=4)\n",
    "print(\"Four Week Lag: \", ac4)\n",
    "ac5 = df['sales'].autocorr(lag=5)\n",
    "print(\"Five Week Lag: \", ac5)\n",
    "ac6 = df['sales'].autocorr(lag=6)\n",
    "print(\"Six Week Lag: \", ac6)\n",
    "ac7 = df['sales'].autocorr(lag=7)\n",
    "print(\"Seven Week Lag: \", ac7)\n",
    "ac8 = df['sales'].autocorr(lag=8)\n",
    "print(\"Eight Week Lag: \", ac8)\n",
    "ac9 = df['sales'].autocorr(lag=9)\n",
    "print(\"Nine Week Lag: \", ac9)\n",
    "ac10 = df['sales'].autocorr(lag=10)\n",
    "print(\"Ten Week Lag: \", ac10)\n",
    "ac11 = df['sales'].autocorr(lag=11)\n",
    "print(\"Eleven Week Lag: \", ac11)\n",
    "ac12 = df['sales'].autocorr(lag=12)\n",
    "print(\"Twelve Week Lag: \", ac12)\n",
    "# These twelve weeks rather strong logs show that autocorrelation is somethng to\n",
    "# be mindful in this model.\n",
    "#####\n",
    "plt.rcParams.update({'ytick.left' : False, 'axes.titlepad':10})\n",
    "fig, axes = plt.subplots(3, 4, figsize=(16, 8), sharex=True, sharey=True, dpi=100)\n",
    "for i, ax in enumerate(axes.flatten()[:12]):\n",
    "    lag_plot(df['sales'], lag=i+1, ax=ax, c='blue')\n",
    "    ax.set_title('Lag ' + str(i+1))\n",
    "fig.suptitle('Lag Plots of Sales', y=1.05)\n",
    "fig.tight_layout()\n",
    "plt.show()\n",
    "# Decreasing linear trend between lag plots demonstrates similar results to\n",
    "# lag scores, suggesting some positive autocorrelation between first week\n",
    "#####\n",
    "train = df['sales'][:105]\n",
    "test = df['sales'][105:]\n",
    "model = auto_arima(train, trace=True, error_action='ignore', suppress_warnings=True)\n",
    "model.fit(train)\n",
    "forecast = model.predict(n_periods=len(test))\n",
    "forecast = pd.DataFrame(forecast,index = test.index,columns=['Prediction'])\n",
    "# SARIMAX MODEL\n",
    "######\n",
    "mod = sm.tsa.statespace.SARIMAX(df['sales'],\n",
    "                                order=(1, 1, 1),\n",
    "                                seasonal_order=(0, 0, 0, 0),\n",
    "                                enforce_stationarity=True,\n",
    "                                enforce_invertibility=True)\n",
    "results = mod.fit()\n",
    "results.plot_diagnostics(figsize=(12, 12))\n",
    "# SARIMAX Diagnostics\n",
    "#####\n",
    "forecast = forecast.squeeze()\n",
    "forecast_accuracy(forecast, test)\n",
    "#####\n",
    "results.summary()\n",
    "#####\n",
    "df_small = df[['week', 'sales']]\n",
    "result = adfuller(df_small.sales.dropna())\n",
    "print('ADF Statistic: %f' % result[0])\n",
    "print('p-value: %f' % result[1])\n",
    "# In this case, we fail the ADF test and will examine differencing\n",
    "# as a method to make our series stationary,\n",
    "######\n",
    "plt.rcParams.update({'figure.figsize':(12,8), 'figure.dpi':120})\n",
    "\n",
    "fig, axes = plt.subplots(3, 2, sharex=False)\n",
    "axes[0, 0].plot(df_small.sales); axes[0, 0].set_title('Original Series')\n",
    "plot_acf(df_small.sales, ax=axes[0, 1])\n",
    "\n",
    "axes[1, 0].plot(df_small.sales.diff()); axes[1, 0].set_title('1st Order Differencing')\n",
    "plot_acf(df_small.sales.diff().dropna(), ax=axes[1, 1])\n",
    "\n",
    "axes[2, 0].plot(df_small.sales.diff().diff()); axes[2, 0].set_title('2nd Order '\n",
    "                                                                    'Differencing')\n",
    "plot_acf(df_small.sales.diff().diff().dropna(), ax=axes[2, 1])\n",
    "fig.tight_layout()\n",
    "plt.show()\n",
    "# By the time the first differencing plot and its correlation plot, we can see\n",
    "# that the model has achieved a decent amount of stationarity.\n",
    "##### Fix Training and Test Data for Remaining Functions\n",
    "df_small = df_small.sort_values(by=['week'])\n",
    "df_model = df_small.set_index('week')\n",
    "df_small = df_small.reset_index()\n",
    "df_small = df_small[['week', 'sales']]\n",
    "train = df_small['sales'][:105]\n",
    "test = df_small['sales'][105:]\n",
    "train_model = df_model['sales'][:186]\n",
    "test_model = df_model['sales'][-16:]\n",
    "# Now we will test the auto_arima model to see if it performs better.\n",
    "#####\n",
    "train = df_model['sales']\n",
    "mod_auto_arima = auto_arima(train, start_p=0, d=1, start_q=0, max_p=5,\n",
    "                            max_d=5, max_q=5, start_P=0, D=1, start_Q=0,\n",
    "                            max_P=5, max_D=5, max_Q=5, m=12, seasonal=True,\n",
    "                            error_action='warn', trace=True,\n",
    "                            suppress_warnings=True, stepwise=True,\n",
    "                            random_state=13, n_fits=50)\n",
    "######\n",
    "train = df['sales'][-16:]\n",
    "test = df['sales'][:16] # missing pass sixteen weeks\n",
    "model = auto_arima(train, trace=True, error_action='ignore', suppress_warnings=True)\n",
    "model.fit(train)\n",
    "forecast = mod_auto_arima.predict(n_periods=16)\n",
    "index = pd.date_range('2022-04-03', '2022-07-17', freq='w')\n",
    "forecast = pd.DataFrame(forecast,index =index,columns=['Prediction'])\n",
    "forecast = forecast.squeeze()\n",
    "forecast_accuracy(forecast, test)\n",
    "Auto_Arima = forecast_accuracy(forecast, test)\n",
    "#####\n",
    "train = df_model['sales'][:186]\n",
    "test = df_model['sales'][186:202]\n",
    "df_temp = df_test.set_index('week')\n",
    "df_temp = df_temp[:16]\n",
    "######\n",
    "plt.plot(train, label='Train')\n",
    "plt.plot(test, label='Test')\n",
    "plt.plot(forecast, label='Forecast')\n",
    "plt.plot(df_temp, label='Actuals')\n",
    "plt.title('Predicted Sales')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Sales')\n",
    "plt.legend(loc='upper left', fontsize=12)\n",
    "fig.autofmt_xdate()\n",
    "plt.show()\n",
    "# Now taking a look at the other predictions compared to actuals.\n",
    "#######\n",
    "df_group = pd.read_csv('G:/My Drive/IN/Data/Forecast_Comparisons/updated_group_forecast2'\n",
    "                       '.csv',\n",
    "                       index_col=\"Date\",parse_dates=True)\n",
    "df_group.astype(float)\n",
    "df_group['actuals'] = test.values"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "######\n",
    "y = [\"season\", \"events\", \"holiday\", \"print_S\", \"search_S\",\n",
    "     \"newsletter\", \"facebook_S\", \"ooh_S\", 'tv_S', 'trend',\n",
    "     'intercept', 'competitor_Sales_B'],\n",
    "x = [-.7, 0.5, 0.6, 0.9, 1.6, 1.7, 2.2, 2.3, 4, 17, 34, 34.5]\n",
    "\n",
    "fig = go.Figure(go.Waterfall(\n",
    "    name = \"2018\", orientation = \"h\",\n",
    "    measure = [\"relative\", \"relative\", \"relative\", \"relative\", \"relative\",\n",
    "               \"relative\", \"relative\", \"relative\", \"relative\", \"relative\",\n",
    "               \"relative\", \"relative\", 'relative'],\n",
    "    y = [\"Seasonality\", \"Events\", \"Holiday\", \"Print\", \"Search\",\n",
    "         \"Newsletter\", \"Facebook\", \"OOH\", 'TV', 'Trending',\n",
    "         'Base', 'Competitor'],\n",
    "\n",
    "    x = [-.7, 0.2, 0.75, 0.95, 1.65, 1.75, 2.25, 2.35, 4.5, 17.5, 34.5, 35, 0],\n",
    "    base = .7,\n",
    "    text = x,\n",
    "    textposition = \"inside\",\n",
    "    decreasing = {\"marker\":{\"color\":\"Maroon\", \"line\":{\"color\":\"#FD9051\", \"width\":0}}},\n",
    "    increasing = {\"marker\":{\"color\":\"#0B3B60\"}},\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title=dict(text=\"Channel Decomposition by Share\",\n",
    "               x=0.5, y=0.93,\n",
    "               font_family=\"Raleway\",\n",
    "               font_size=16,\n",
    "               font_color=\"black\"),\n",
    "    height = 600,\n",
    "    width = 800,\n",
    "    plot_bgcolor = 'white',\n",
    "    paper_bgcolor='white',\n",
    "    waterfallgroupgap = 0.0,\n",
    "    font_family = 'Raleway',\n",
    "    font_color='rgb(99, 112, 122)',\n",
    "    xaxis={'visible': False, 'showticklabels': True})\n",
    "\n",
    "fig.update_yaxes(showline=False, linewidth=2, linecolor='grey', gridcolor='grey')\n",
    "fig.update_xaxes(showgrid=False)\n",
    "fig.update_yaxes(showgrid=False,showline = False, linecolor = '#000')\n",
    "\n",
    "fig['layout']['xaxis']['autorange'] = \"reversed\"\n",
    "fig.show()\n",
    "\n",
    "fig.write_image('Plots_Output/channel_Decomp.png')\n",
    "######\n",
    "\n",
    "categories = ['tv_S - ', 'search_clicks_P - ', 'print_S - ', 'ooh_S - ', 'facebook_I - ']\n",
    "Spend_Share = [52,15,33,91,53]\n",
    "Effect_Share = [38,51,51,42,87]\n",
    "Total_ROI = [73,62,70,95,76]\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Line(\n",
    "        orientation = \"h\",\n",
    "        x=Total_ROI,\n",
    "        y=categories,\n",
    "        marker_size = 5,\n",
    "        marker_color='rgb(253, 144, 81)',\n",
    "        name='ROI',\n",
    "    ))\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Bar(\n",
    "        orientation = \"h\",\n",
    "        x=Effect_Share,\n",
    "        y=categories,\n",
    "        marker_color='rgb(95, 146, 191)',\n",
    "        name='Effect Share'\n",
    "    ))\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Bar(\n",
    "        orientation = \"h\",\n",
    "        x=Spend_Share,\n",
    "        y=categories,\n",
    "        marker_color='rgb(11, 59, 96)',\n",
    "        name='Spend Share'\n",
    "    ))\n",
    "\n",
    "fig.update_layout(legend=dict(yanchor=\"top\", y=-.1, xanchor=\"left\", x=0.2, orientation='h'),\n",
    "                  # title=dict(text=\"This is my title\",\n",
    "                  #            x=0.5, y=0.93,\n",
    "                  #            font_family=\"Raleway\",\n",
    "                  #            font_size=16,\n",
    "                  #            font_color=\"black\"),\n",
    "                  # font_family=\"Arial\",\n",
    "                  # font_family=\"Balto\",\n",
    "                  # font_family=\"Courier New\",\n",
    "                  # font_family=\"Droid Sans\",\n",
    "                  # font_family=\"Droid Serif\",\n",
    "                  # font_family=\"Droid Sans Mono\",\n",
    "                  # font_family=\"Gravitas One\",\n",
    "                  # font_family=\"Old Standar TT\",\n",
    "                  # font_family=\"Open Sans\",\n",
    "                  # font_family=\"Overpass\",\n",
    "                  # font_family=\"PT Sans Narrow\",\n",
    "                  font_family=\"Raleway\",\n",
    "                  # font_family=\"Times New Roman\",\n",
    "                  font_size=12,\n",
    "                  paper_bgcolor='white',\n",
    "                  plot_bgcolor = 'white',\n",
    "                  title_x = .5,\n",
    "                  title_text = \"Spend Share vs. Effect Share with ROI\"\n",
    "                               \"<br><sup>R^2 = .928, RMSE = .061, DECOMP.RSSD = .026<sup>\",\n",
    "                  showlegend = True,\n",
    "                  height = 400,\n",
    "                  width = 800,\n",
    "                  font_color='rgb(99, 112, 122)')\n",
    "\n",
    "fig.update_yaxes(showline=False, linewidth=2, linecolor='grey', gridcolor='grey')\n",
    "fig.update_xaxes(showgrid=False)\n",
    "fig.update_yaxes(showgrid=False,showline = False, linecolor = '#000')\n",
    "\n",
    "fig.show()\n",
    "\n",
    "# fig.write_image('Plots_Output/arial.png')\n",
    "# fig.write_image('Plots_Output/balto.png')\n",
    "# fig.write_image('Plots_Output/courier_new.png')\n",
    "# fig.write_image('Plots_Output/droid_sans.png')\n",
    "# fig.write_image('Plots_Output/droid_serif.png')\n",
    "# fig.write_image('Plots_Output/droid_sans_mono.png')\n",
    "# fig.write_image('Plots_Output/gravitas.png')\n",
    "# fig.write_image('Plots_Output/old_standard.png')\n",
    "# fig.write_image('Plots_Output/open_sans.png')\n",
    "# fig.write_image('Plots_Output/overpass.png')\n",
    "# fig.write_image('Plots_Output/pt_sans.png')\n",
    "# fig.write_image('Plots_Output/raleway.png')\n",
    "# fig.write_image('Plots_Output/times_new_roman.png')\n",
    "#######\n",
    "categories = ['Television - ', 'Searches Spend - ', 'Print Spend - ', 'OOH Spend - ', 'Newsletter Spend -', 'FB Spend - ']\n",
    "Spend_Share = [52,15,33,91,53]\n",
    "Effect_Share = [38,51,51,42,87]\n",
    "Total_ROI = [73,62,70,95,76]\n",
    "values = '67.7%', \"70.3%\", '70.5%', '24.0%', '37.9%', '67.3%'\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Line(\n",
    "        orientation = \"h\",\n",
    "        x=Total_ROI,\n",
    "        y=categories,\n",
    "        marker_size = 5,\n",
    "        marker_color='rgb(253, 144, 81)',\n",
    "        name='ROI',\n",
    "    ))\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Bar(\n",
    "        orientation = \"h\",\n",
    "        x=values,\n",
    "        y=categories,\n",
    "        marker_color='rgb(95, 146, 191)',\n",
    "        name='Effect Share'\n",
    "    ))\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Bar(\n",
    "        orientation = \"h\",\n",
    "        x=Spend_Share,\n",
    "        y=categories,\n",
    "        marker_color='rgb(11, 59, 96)',\n",
    "        name='Spend Share'\n",
    "    ))\n",
    "\n",
    "fig.update_layout(legend=dict(yanchor=\"top\", y=-.1, xanchor=\"left\", x=0.2, orientation='h'),\n",
    "                  paper_bgcolor='white',\n",
    "                  plot_bgcolor = 'white',\n",
    "                  title_x = .5,\n",
    "                  title_text = \"Geometric Adstock: Fixed Rate Over Time\",\n",
    "                  # title_text = \"Spend Share vs. Effect Share with ROI \"\n",
    "                  #              \"<br><sup>R^2 = .928, RMSE = .061, DECOMP.RSSD = .026<sup>\",\n",
    "                  showlegend = False,\n",
    "                  height = 400,\n",
    "                  width = 800,\n",
    "                  waterfallgroupgap = 0.0,\n",
    "                  font_family = 'inter',\n",
    "                  font_color='black')\n",
    "\n",
    "fig.update_layout(plot_bgcolor = 'rgb(240, 244, 248)',\n",
    "                  title = \"Channel Decomposition by Share\", showlegend = False,\n",
    "                  height = 600,\n",
    "                  width = 800,\n",
    "                  xaxis=dict(\n",
    "                      # title_text=\"Y-axis Title\",\n",
    "                      # ticktext=[\"Very long label\", \"long label\", \"3\", \"label\"],\n",
    "                      tickvals=[0, 20, 40, 60, 80, 100],\n",
    "                      tickmode=\"array\",\n",
    "                      titlefont=dict(size=30),\n",
    "                  ),\n",
    "                  waterfallgroupgap = 0.0,\n",
    "                  font_family = 'Raleway')\n",
    "\n",
    "fig.update_yaxes(showline=True, linewidth=2, linecolor='grey', gridcolor='grey')\n",
    "fig.update_xaxes(showgrid=False)\n",
    "\n",
    "\n",
    "fig.show()\n",
    "\n",
    "fig.update_yaxes(showline=True, linewidth=2, linecolor='white', gridcolor='ff')\n",
    "fig.update_xaxes(showgrid=False)\n",
    "fig.update_yaxes(showgrid=False,showline = False, linecolor = '#000')\n",
    "\n",
    "# fig.write_image('images/share_v_effect.png')\n",
    "from fontTools.misc.symfont import y\n",
    "\n",
    "######\n",
    "fig.update_layout(legend=dict(yanchor=\"top\", y=-.1, xanchor=\"left\", x=0.2, orientation='h'),\n",
    "                  paper_bgcolor='white',\n",
    "                  plot_bgcolor = 'white',\n",
    "                  title_x = .5,\n",
    "                  title_text = \"Spend Share vs. Effect Share with ROI \"\n",
    "                               \"<br><sup>R^2 = .928, RMSE = .061, decomp.rssd = .026<sup>\",\n",
    "                  showlegend = True,\n",
    "                  height = 800,\n",
    "                  width = 800,\n",
    "                  xaxis=dict(\n",
    "                      # title_text=\"Y-axis Title\",\n",
    "                      # ticktext=[\"Very long label\", \"long label\", \"3\", \"label\"],\n",
    "                      text=[0, 20, 40, 60, 80, 100],\n",
    "                      # tickmode=\"array\",\n",
    "                      # titlefont=dict(size=30),\n",
    "                  ),\n",
    "                  fig.update_yaxes(showline=True, linewidth=2, linecolor='grey', gridcolor='grey')\n",
    "fig.update_xaxes(showgrid=False)\n",
    "fig.update_yaxes(showgrid=False,showline = False, linecolor = '#000'),\n",
    "waterfallgroupgap = 0.0,\n",
    "font_family = 'inter',\n",
    "x = [-.7, 0.2, 0.75, 0.95, 1.65, 1.75, 2.25, 2.35, 4.5, 17.5, 34.5, 35, 0],\n",
    "base = -.7,\n",
    "text = y,\n",
    "textposition = \"outside\",\n",
    "decreasing = {\"marker\":{\"color\":\"Maroon\", \"line\":{\"color\":\"#FD9051\", \"width\":0}}},\n",
    "increasing = {\"marker\":{\"color\":\"#0B3B60\"}},\n",
    "fig.update_yaxes(showgrid=False,showline = False, linecolor = '#000'),\n",
    "fig.update_yaxes(showline=True, linewidth=2, linecolor='grey', gridcolor='grey'),\n",
    "fig.update_xaxes(showgrid=False),\n",
    "fig.update_yaxes(showgrid=False,showline = False, linecolor = '#000'))\n",
    "\n",
    "fig.update_yaxes(showline=True, linewidth=2, linecolor='grey', gridcolor='grey'),\n",
    "fig.update_xaxes(showgrid=False),\n",
    "fig.update_yaxes(showgrid=False,showline = False, linecolor = '#000'))\n",
    "fig.show()\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = pd.read_csv('C:/Users/norri/Desktop/native.csv', parse_dates=['DATE'])\n",
    "# Wanted to get summaries before I broke them into groups. I found four missing values, so I just did fillna(0)\n",
    "df.info()\n",
    "# df.drop(['brands_auto_s'], axis=1, inplace=True)\n",
    "df.fillna(0, inplace=True)\n",
    "# I checked for a column with too many zeroes (I haven't decided a threshold for that yet).\n",
    "print((df == 0).sum() / len(df))\n",
    "# brands_category_s is all zeroes and will be dropped\n",
    "# df.drop(['brands_category_s'], axis=1, inplace=True)\n",
    "print('Missing Values')\n",
    "print(df.isnull().sum() / len(df) * 100)\n",
    "df_spend = df[['display_s', 'brands_branded_s', 'brands_competitive_s', 'products_branded_s', 'products_category_s', 'products_competitive_s', 'products_auto_s']]\n",
    "df_no_dep = df.drop(['units', 'revenue', 'DATE'], axis=1)\n",
    "df_no_date = df[['display_s', 'brands_branded_s', 'brands_competitive_s', 'products_branded_s', 'products_category_s', 'products_competitive_s', 'products_auto_s', 'units', 'revenue']]\n",
    "# These results make sense because product_category_s has quite a few zeroes.\n",
    "df.nunique()\n",
    "# Looking for sufficient variance here. With units, they tend to have less variance.\n",
    "# We can verify variance with the KDE plots overlaying the histograms.\n",
    "df.describe()\n",
    "\n",
    "######\n",
    "fig, axes = plt.subplots(3, 3, figsize=(14, 14))\n",
    "sns.histplot(df_no_date['display_s'], stat='count', kde=True, bins=int(180/5), ax=axes[0, 0], color = 'blue')\n",
    "sns.histplot(df_no_date['brands_branded_s'], kde=True, bins=int(180/5), ax=axes[0, 1], color = 'blue')\n",
    "sns.histplot(df_no_date['brands_competitive_s'], kde=True, bins=int(180/5), ax=axes[0, 2], color = 'blue')\n",
    "sns.histplot(df_no_date['products_branded_s'], stat='count', kde=True, bins=int(180/5), ax=axes[1, 0], color = 'blue')\n",
    "sns.histplot(df_no_date['products_category_s'], kde=True, bins=int(180/5), ax=axes[1, 1], color = 'blue')\n",
    "sns.histplot(df_no_date['products_competitive_s'], kde=True, bins=int(180/5), ax=axes[1, 2], color = 'blue')\n",
    "sns.histplot(df_no_date['products_auto_s'], stat='count', kde=True, bins=int(180/5), ax=axes[2, 0], color = 'blue')\n",
    "sns.histplot(df_no_date['units'], kde=True, bins=int(180/5), ax=axes[2, 1], color = 'blue')\n",
    "sns.histplot(df_no_date['revenue'], kde=True, bins=int(180/5), ax=axes[2, 2], color = 'blue')\n",
    "# The spread of the KDE plot, along with the data above, suggests the variance is sufficient.\n",
    "# Most data is close to normal with a skew or slight bimodality, with the exception of\n",
    "# products_category_s, which has a large number of zeros.\n",
    "#####\n",
    "corr = df_no_date.corr(method=\"pearson\").round(2)\n",
    "mask = np.triu(np.ones_like(corr, dtype=bool))\n",
    "f, ax = plt.subplots(figsize=(15, 15))\n",
    "cmap = sns.color_palette('icefire', as_cmap=True)\n",
    "sns.heatmap(corr, annot=True, mask=mask, cmap=cmap, vmax=1, vmin=-1, center=0,\n",
    "square=True, linewidths=.5, cbar_kws={\"shrink\": .5})\n",
    "# Both revenue and units a have weak to moderate correlation with brands_competitive_s and products_branded_s,\n",
    "# weak correlations with brands_branded_s and products_category_s, and little to no correlation\n",
    "# with display_s, products_competitive_s, and products_auto_s.\n",
    "corr.describe()\n",
    "# corr = df_spend.corr(method=\"pearson\").round(2)\n",
    "# mask = np.triu(np.ones_like(corr, dtype=bool))\n",
    "# f, ax = plt.subplots(figsize=(15, 15))\n",
    "# cmap = sns.color_palette('icefire', as_cmap=True)\n",
    "# sns.heatmap(corr, annot=True, mask=mask, cmap=cmap, vmax=1, vmin=-1, center=0,\n",
    "#             square=True, linewidths=.5, cbar_kws={\"shrink\": .5})\n",
    "# corr.describe()\n",
    "corr = df_no_dep.corr(method=\"pearson\").round(2)\n",
    "mask = np.triu(np.ones_like(corr, dtype=bool))\n",
    "f, ax = plt.subplots(figsize=(15, 15))\n",
    "cmap = sns.color_palette('icefire', as_cmap=True)\n",
    "sns.heatmap(corr, annot=True, mask=mask, cmap=cmap, vmax=1, vmin=-1, center=0,\n",
    "square=True, linewidths=.5, cbar_kws={\"shrink\": .5})\n",
    "# Brands_branded_s has a weak to moderate correlation with brands_competitive_s and products_branded_s.\n",
    "#         Products_branded_s has a weak positive correlation with products_category_s, and products_competitive_s has a\n",
    "# weak negative correlation to brands_competitive_s.\n",
    "corr.describe()\n",
    "# This section checks for outliers and approximates some idea of how extreme they are.\n",
    "# However, in Robyn, its converge() function that runs within the robyn_run() model\n",
    "# automatically winsorizes (adjusts upper and lower bounds of data input). This is just\n",
    "# a diagnostic if the model output is abnormal.\n",
    "######\n",
    "lower_q = df_no_date.quantile(0.25)\n",
    "upper_q = df_no_date.quantile(0.75)\n",
    "iqr = upper_q - lower_q\n",
    "lower_bound = lower_q - (1.5 * iqr)\n",
    "upper_bound = upper_q + (1.5 * iqr)\n",
    "outliers = df_no_date[(df_no_date < lower_bound) | (df_no_date > upper_bound)]\n",
    "print(df_no_date.mean())\n",
    "print(outliers.mean())\n",
    "print(outliers.count())\n",
    "# Product_category_s has the most outliers, though it has so many zeros that\n",
    "# it lowers the mean in comparison to times of regular spend.\n",
    "# There is nothing to suggest data entry errors or data corruption, so no\n",
    "# correction is warranted.\n",
    "# These two modules make html reports that are quite good and can supplement for an EDA if the EDA is not available.\n",
    "# In addition, I chose them because they do a good Time Series analysis and export it.\n",
    "######\n",
    "profile_y = ydata_prof(df, title=\"Initial EDA\")\n",
    "profile_y.to_file(\"basic_eda.html\")\n",
    "########\n",
    "profile_pd = pd_prof(df, tsmode=True, sortby=\"DATE\", title='Time Series EDA')\n",
    "profile_pd.to_file(\"time_series_eda.html\")"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
