{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ece116d7-ba70-46e7-a300-7c20b1796d9e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Item Attribution Recommender Project Testing\n",
    "\n",
    "Test out the capabilities of the API on Azure OpenAI Studio\n",
    "\n",
    "Here's some resources:\n",
    "- [Video on using Python with the studio](https://www.youtube.com/watch?v=gNCk8tW5QS8)\n",
    "- [Azure Documentation](https://learn.microsoft.com/en-us/azure/ai-services/openai/tutorials/embeddings?tabs=command-line)\n",
    "- [Medium article tutorial](https://medium.com/microsoftazure/querying-structured-data-with-azure-openai-e59ee43867e5)\n",
    "- [OpenAI API Documentation on Fine-Tuning](https://platform.openai.com/docs/guides/fine-tuning)\n",
    "- [Microsoft documentation on Bringing Your Own Data in Python](https://learn.microsoft.com/en-us/azure/ai-services/openai/use-your-data-quickstart?tabs=command-line&pivots=programming-language-python#create-the-python-app)\n",
    "- [Azure SDK Community Standup 9/14/23](https://www.youtube.com/watch?v=hFBXtGqTo7A)\n",
    "- [Azure OpenAI REST API Documentation](https://learn.microsoft.com/en-us/azure/ai-services/openai/reference)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# # Imports and API setup\n",
    "# import openai\n",
    "# import tiktoken\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# import requests\n",
    "# import json\n",
    "# import dotenv\n",
    "# from azure.identity import DefaultAzureCredential\n",
    "# \n",
    "# # May need to update API version and deployment for the custom data section\n",
    "# openai.api_key = dbutils.secrets.get(scope=\"openai\", key=\"access-token\") # Added using the Databricks CLI, see below for details\n",
    "# openai.api_base = dbutils.secrets.get(scope=\"openai\", key=\"endpoint\") \n",
    "# openai.api_type = 'azure'\n",
    "# openai.api_version = '2023-05-15'\n",
    "# \n",
    "# # Switch out deployments HERE\n",
    "# deployment_name='gpt-4-item-recommender' #This will correspond to the custom name you chose for your deployment when you deployed a model. "
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "To create/utilize secrets, you'll need to use the Databricks CLI. [Here's](https://learn.microsoft.com/en-us/azure/databricks/dev-tools/cli/install) how to install the CLI on your local machine (I directly used a ZIP of the CLI exe). Once the CLI is installed, you can create an access token in your Databricks user profile in the top right corner of the workspace. Add this profile in the CLI by following this [page](https://learn.microsoft.com/en-us/azure/databricks/dev-tools/cli/authentication). \n",
    "\n",
    "Once all that's set up, you can use the help command on the `databricks secrets put-secrets` and `databricks secrets create-scope` commands to add secrets for use in your notebooks. Don't follow the Microsoft syntax for these commands, as they use a different version of the CLI."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Send a completion call to generate an answer\n",
    "# Make sure to use the 2023-05-15 version of the API for this cell\n",
    "\n",
    "#start_phrase = 'Python can be used '\n",
    "#response = openai.Completion.create(engine=deployment_name, prompt=start_phrase, max_tokens=100)\n",
    "#text = response['choices'][0]['text'].replace('\\n', '').replace(' .', '.').strip()\n",
    "#print(start_phrase+text)"
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# To add the unfi data, use the Azure Cognitive Search Index \"unfi-sample\" instead of the data directly in Databricks\n",
    "# Set up utility functions\n",
    "# These utility functions came directly from the Azure SDK Community Standup from 9/14/23, see above for link\n",
    "#openai.api_type = 'azure'\n",
    "#openai.api_version = '2023-08-01-preview' #Following a tutorial in terms of versioning - switch this on in case you use this cell"
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# def setup_byod(deployment_id: str) -> None:\n",
    "#     \"\"\" Sets up the OpenAI Python SDK to use your own data for the chat endpoint \n",
    "# \n",
    "#        :param deployment_id: The deployment ID for the model to use with your own data.\n",
    "# \n",
    "#        To remove this configuration, simply set openai.requestssession to None.\n",
    "#     \"\"\"\n",
    "#     class BringYourOwnDataAdapter(requests.adapters.HTTPAdapter):\n",
    "# \n",
    "#         def send(self, request, **kwargs):\n",
    "#             request.url = f\"{openai.api_base}/openai/deployments/{deployment_id}/extensions/chat/completions?api-version={openai.api_version}\"\n",
    "#             return super().send(request, **kwargs)\n",
    "# \n",
    "#     session = requests.Session()\n",
    "#     session.mount(\n",
    "#         prefix=f\"{openai.api_base}/openai/deployments/{deployment_id}\",\n",
    "#         adapter=BringYourOwnDataAdapter()\n",
    "#     )\n",
    "# \n",
    "#     openai.requestssession = session "
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# SDK Configuration\n",
    "# setup_byod(deployment_name)\n",
    "\n",
    "# Set up the chat functionality\n",
    "#def ask_question(question: str, show_context: bool = False) -> str:\n",
    "    #completion = openai.ChatCompletion.create(\n",
    "     #   messages=[{\"role\": \"user\", \"content\": question}],\n",
    "      #  temperature=0.1,  # Keep temp low for deterministic answers\n",
    "       # deployment_id=deployment_name,\n",
    "        #dataSources=[\n",
    "         #   {\n",
    "          #      \"type\": \"AzureCognitiveSearch\",\n",
    "           #     \"parameters\": {\n",
    "            #        \"endpoint\": dbutils.secrets.get(\n",
    "             #           scope=\"openai\", key=\"search-endpoint\"\n",
    "              #      ),\n",
    "               #     \"key\": dbutils.secrets.get(scope=\"openai\", key=\"search-key\"),\n",
    "                #    \"indexName\": \"unfi-sample\",\n",
    "                #},\n",
    "            #}\n",
    "        #],\n",
    "    #)\n",
    "    #if show_context:\n",
    "    #    print(\"Context:\")\n",
    "    #    print(\n",
    "    #        json.dumps(\n",
    "     #           json.loads(completion.choices[0].message.context.messages[0].content),\n",
    "      #          indent=2,\n",
    "       #     )\n",
    "       # )\n",
    "    #return f\"Answer: {completion.choices[0].message.content}\""
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The example above is working for simple queries of a sample of the dataset in txt format, but not for the pattern completion we need of this model. Instead of continuing with the BYOD chat completion, let's instead perform some prompt engineering and [few shot prompting](https://www.promptingguide.ai/techniques/fewshot) to try and get the model to recognize the pattern."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Bring in data from warehouse\n",
    "# df_unfi = spark.read.table(\"unfi_item_list\")\n",
    "# \n",
    "# # Separate this table into one free of null values and one containing nulls\n",
    "# df_unfi_train = df_unfi.filter(df_unfi[\"ADV_Category\"] != \"NA\")\n",
    "# \n",
    "# df_unfi_test = df_unfi.filter(df_unfi[\"ADV_Category\"] == \"NA\")"
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# # Convert a sample of the above dataframe to a str\n",
    "# unfi_sample = str(df_unfi_train.head(45))\n",
    "# \n",
    "# # Create a \"validation\" set to test the AI's ability to recognize the pattern from the first sample\n",
    "# unfi_sample_val = str(df_unfi_test.select(\"SVItemCD\", \"SVBrand\", \"SVDescrip\", \"VendPack\", \"SVSize\", \"SVPack\", \"UPCItem\", \"UPCCase\").tail(5))"
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Call the same model via the chat completion API\n",
    "# First, write out the prompt input\n",
    "#setup_byod(deployment_name)\n",
    "\n",
    "# prompt = f\"\"\"\n",
    "# The following data is a sample of a dataset of product attributes called the UNFI Item List. {unfi_sample} \n",
    "# The first eight columns are used to fill in the last 9 columns. Can you autocomplete the last 9 columns of the following sample from the UNFI Item List:\n",
    "# {unfi_sample_val} and format your answer into a table?\n",
    "# \"\"\"\n",
    "# \n",
    "# response = openai.ChatCompletion.create(\n",
    "#     engine=deployment_name,\n",
    "#     messages=[\n",
    "#         {\"role\": \"system\", \"content\": \"You are a model used to fill in missing data.\"}, # Try out different language to avoid imputation tactics on the part of the model\n",
    "#         {\"role\": \"user\", \"content\": prompt}\n",
    "#         ]\n",
    "#     \n",
    "# )\n",
    "# \n",
    "# print(f\"Answer: {response['choices'][0]['message']['content']}\")"
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Validation set for the above query\n",
    "# display(df_unfi_test.tail(5))"
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Results from this are better than above - the model is actually recognizing the data and attempting to impute nulls. However, the results are inconsistent in their quality, so we'll have to keep experimenting with prompt engineering as that's been the most successful method so far."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Try out splitting the task into sub-prompts\n",
    "# Imputation of Category and SubCategory\n",
    "# prompt = f\"\"\"\n",
    "# The following data is a sample of a dataset of product attributes called the UNFI Item List. {unfi_sample} \n",
    "# The first eight columns are used to fill in the last 9 columns. Can you fill in the null values in the ADV_Category and ADV_SubCategory columns of the following sample from the UNFI Item List:\n",
    "# {unfi_sample_val} and format your answer into a table? The ADV_Category column is based on the SVDescrip column and the ADV_SubCategory column is based on the SVBrand column.\n",
    "# \"\"\"\n",
    "# \n",
    "# response = openai.ChatCompletion.create(\n",
    "#     engine=deployment_name,\n",
    "#     messages=[\n",
    "#         {\"role\": \"system\", \"content\": \"You are a model used to fill in missing data.\"}, # Try out different language to avoid imputation tactics on the part of the model\n",
    "#         {\"role\": \"user\", \"content\": prompt}\n",
    "#         ]\n",
    "#     \n",
    "# )\n",
    "# \n",
    "# print(f\"Answer: {response['choices'][0]['message']['content']}\")"
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Imputation of the rest of the columns\n",
    "# prompt = f\"\"\"\n",
    "# The following data is a sample of a dataset of product attributes called the UNFI Item List. {unfi_sample} \n",
    "# The first eight columns are used to fill in the last 7 columns. Can you autocomplete the last 7 columns of the following sample from the UNFI Item List:\n",
    "# {unfi_sample_val} and format your answer into a table?\n",
    "# \"\"\"\n",
    "# \n",
    "# response = openai.ChatCompletion.create(\n",
    "#     engine=deployment_name,\n",
    "#     messages=[\n",
    "#         {\"role\": \"system\", \"content\": \"You are a model used to fill in missing data.\"}, # Try out different language to avoid imputation tactics on the part of the model\n",
    "#         {\"role\": \"user\", \"content\": prompt}\n",
    "#         ]\n",
    "#     \n",
    "# )\n",
    "# \n",
    "# print(f\"Answer: {response['choices'][0]['message']['content']}\")"
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Validation set for the above queries\n",
    "# display(df_unfi_test.tail(5))"
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# display(unfi_sample_val)"
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Testing API Changes/2nd Code Draft\n",
    "\n",
    "The code below is a more refined and updated version of the stuff above. I'll keep all of the preceding code for posterity's sake, but know that the code below is closer to the final version. "
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# # New data import - use Pandas instead\n",
    "# # Bring in data from warehouse\n",
    "# df_unfi = spark.read.table(\"unfi_item_list\")\n",
    "# \n",
    "# # Convert to Pandas dataframe\n",
    "# df_unfi = df_unfi.toPandas()\n",
    "# \n",
    "# # Replace na with actual null values\n",
    "# df_unfi = df_unfi.replace('NA', np.nan)\n",
    "# #df_unfi.isnull().sum()\n",
    "# \n",
    "# # Separate this table into one free of null values and one containing nulls\n",
    "# df_unfi_test = df_unfi[~(df_unfi.notna().all(axis=1))]\n",
    "# \n",
    "# df_unfi_train = df_unfi[df_unfi.notna().all(axis=1)]\n",
    "# \n",
    "# # Convert a sample of the above dataframe to a str\n",
    "# unfi_sample = df_unfi_train.head(800).to_string(sparsify=False, justify=\"center\") # Keep this at 800 max while using gpt-4 turbo\n",
    "# \n",
    "# # Create a \"validation\" set to test the AI's ability to recognize the pattern from the first sample\n",
    "# unfi_sample_val = df_unfi_test.head(20).to_string(sparsify=False, justify=\"center\")"
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Between the timeout errors and the model running out of output tokens, it looks like the limit for output is **15 rows at a time**. This means we'll have to implement a loop, maybe we can parallelize the task so we can run multiple loops at once. Trying to get outputs beyond 15 rows makes the run time increase exponentially - it goes from a few minutes to over an hour. This might be because trying to output over the token limit makes the model hang until it times out. So maybe another solution here is to get a quota increase on the output tokens like we did with the input tokens."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# # Call the same model via the chat completion API\n",
    "# # Also add more detail in the prompt on what columns map to each other\n",
    "# # To-do: Create a user defined method of mapping columns\n",
    "# prompt = f\"\"\"\n",
    "# The following data is a sample of a dataset of product attributes called the UNFI Item List: {unfi_sample} \n",
    "# The missing item attribution values need to be filled in. Here's how the columns map to each other:\n",
    "# the ADV_Category column is based on the SVItemCD column,\n",
    "# the ADV_Brand column is based on the SVBrand column,\n",
    "# the ADV_SubCategory column is inferred using the SVBItemCD and SVItemDescrip columns,\n",
    "# the ADV_Brand column is based on the SVBrand column,\n",
    "# the ADV_ItemDescrip column is based on the SVDescrip column,\n",
    "# the ADV_Size column is based on the SVSize column,\n",
    "# the ADV_Store_Pack cloumn is based on the VendPack column,\n",
    "# the ADV_ItemUPC column is copied from the UPCItem column and should not be formatted in scientific notation,\n",
    "# the ADV_CaseUPC10 column is exactly the same as the ADV_ItemUPC column and should not be formatted in scientific notation,\n",
    "# and the RptLvlFilter column values can be left null.\n",
    "# \n",
    "# Autocomplete the last 9 columns of the following validation sample from the UNFI Item List:\n",
    "# {unfi_sample_val} and format your answer into a table using | to separate columns. Make sure your output starts with '| SVItemCD |' and ends with '| EXCLUDE      |'. Do not insert a blank row under the header in the output. \n",
    "# \"\"\"\n",
    "# \n",
    "# response = openai.ChatCompletion.create(\n",
    "#     engine=deployment_name,\n",
    "#     request_timeout=3600, # Up the time to timeout significantly - this is in seconds\n",
    "#     temperature=1, # Try raising this to suppress the \"I can't generate data\" response\n",
    "#     messages=[\n",
    "#         {\"role\": \"system\", \"content\": \"You are a model used to fill in missing data.\"}, # Try out different language to avoid imputation tactics on the part of the model\n",
    "#         {\"role\": \"user\", \"content\": prompt}\n",
    "#         ]\n",
    "#     \n",
    "# )\n",
    "# \n",
    "# print(f\"Answer: {response['choices'][0]['message']['content']}\")"
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "If you need help with the Chat Completion API and Python function, [here](https://platform.openai.com/docs/api-reference/introduction?lang=python) is a reference from OpenAI and [here's](https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/chatgpt?tabs=python&pivots=programming-language-chat-completions) one from Microsoft. Here's a good [Stackoverflow thread](https://stackoverflow.com/questions/75787638/openai-gpt-3-api-error-request-timed-out) on the timeout issues and how to turn the above code into production code."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# # Take the response from the bot and transfer the data into an exportable format\n",
    "# output = response['choices'][0]['message']['content']\n",
    "# \n",
    "# # Trim out the non-tabular data\n",
    "# output_start = output.find('| SVItemCD |')\n",
    "# output_end = output.rfind('| EXCLUDE    |')\n",
    "# \n",
    "# # StringIO converts the trimmed response string to a file pandas read_csv can convert to a dataframe\n",
    "# output_df = pd.read_csv(StringIO(output[output_start:output_end]), delimiter='|', index_col=[0], skiprows=[1]) # Skip the first row where the model keeps putting dash marks\n",
    "# \n",
    "# display(output_df)"
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# # Validation set for the above query\n",
    "# display(df_unfi_test.head(5))"
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Testing out the Model on the AWG dataset\n",
    "\n",
    "This dataset doesn't have any natural null values to impute, so we'll need to create nulls artifically using numpy. Source [here](https://cmdlinetips.com/2019/05/how-to-randomly-add-nan-to-pandas-dataframe/)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Bring in AWG data\n",
    "df_awg = spark.read.table(\"awg_item_list\")\n",
    "\n",
    "# Convert to Pandas dataframe\n",
    "df_awg = df_awg.toPandas()\n",
    "\n",
    "# Create artifical null values - the number of nulls is higher here than in the UNFI dataset to stress test the model a bit\n",
    "# Make sure the nulls only occur in the ADV columns though!\n",
    "# Define the columns where you want to introduce NaNs  \n",
    "target_columns = ['ADV_Brand', 'ADV_Category', 'ADV_SubCategory', 'ADV_ItemDescrip', 'ADV_ItemUPC', 'ADV_CaseUPC10', 'ADV_Size', 'ADV_StorePack', 'RptLvlFilter']  \n",
    "  \n",
    "# Define the proportion of NaNs you want in your DataFrame  \n",
    "nan_proportion = 0.4  # 40% NaNs  \n",
    "  \n",
    "# Calculate the number of values to replace with NaN for each column  \n",
    "nan_count_per_column = {column: int(df_awg.shape[0] * nan_proportion) for column in target_columns}  \n",
    "  \n",
    "# Randomly choose indices to be replaced with NaN for each target column  \n",
    "for column in target_columns:  \n",
    "    nan_indices = np.random.choice(df_awg.index, nan_count_per_column[column], replace=False)  \n",
    "    df_awg.loc[nan_indices, column] = np.nan  \n",
    "\n",
    "# Separate this table into one free of null values and one containing nulls\n",
    "df_awg_test = df_awg[~(df_awg.notna().all(axis=1))]\n",
    "\n",
    "df_awg_train = df_awg[df_awg.notna().all(axis=1)]\n",
    "\n",
    "# Convert a sample of the above dataframe to a str\n",
    "awg_sample = df_awg_train.head(800).to_string(sparsify=False, justify=\"center\") # Keep this at 200 while using the 32k gpt 4 model\n",
    "\n",
    "# Create a \"validation\" set to test the AI's ability to recognize the pattern from the first sample\n",
    "awg_sample_val = df_awg_test.head(5).to_string(sparsify=False, justify=\"center\")"
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# # Model prompt\n",
    "# prompt = f\"\"\"\n",
    "# The following data is a sample of a dataset of product attributes called the AWG Item List: {awg_sample} \n",
    "# The missing item attribution values need to be filled in. Here's how the columns map to each other:\n",
    "# the ADV_Category column is based on the Category Name column,\n",
    "# the ADV_Brand column is based on the Brand column,\n",
    "# the ADV_SubCategory column is inferred using the Sub Category Name column,\n",
    "# the ADV_ItemDescrip column is based on the Item Description column,\n",
    "# the ADV_Size column is based on the Size column,\n",
    "# the ADV_Store_Pack cloumn is based on the Store Pack column,\n",
    "# the ADV_ItemUPC column is copied from the UPCItem column and should not be formatted in scientific notation,\n",
    "# the ADV_CaseUPC10 column is exactly the same as the UPCCase column and should not be formatted in scientific notation,\n",
    "# and the RptLvlFilter column values can be left null.\n",
    "# \n",
    "# Autocomplete the last 9 columns of the following validation sample from the AWG Item List:\n",
    "# {awg_sample_val} and format your answer into a table using | to separate columns. Make sure your output starts with '| Item Code |' and ends with '| INCLUDE      |'.\n",
    "# \"\"\"\n",
    "# \n",
    "# response = openai.ChatCompletion.create(\n",
    "#     engine=deployment_name,\n",
    "#     temperature=1, # Try raising this to suppress the \"I can't generate data\" response\n",
    "#     messages=[\n",
    "#         {\"role\": \"system\", \"content\": \"You are a model used to fill in missing data.\"}, # Try out different language to avoid imputation tactics on the part of the model\n",
    "#         {\"role\": \"user\", \"content\": prompt}\n",
    "#         ]\n",
    "# \n",
    "# )\n",
    "# \n",
    "# print(f\"Answer: {response['choices'][0]['message']['content']}\")"
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Final Draft on Dev Environment\n",
    "\n",
    "After meeting with the team on 1/22/24, I'm trying a new approach to the imputation where the data will be split column-wise and fed into the model separately. This will hopefully avoid the token/timeout issues we've been experiencing lately and allow us to feed the model more input columns for training. It also makes sense from a parallelization standpoint; we can run 6 of these column API calls at the same time to save time on the overall processing in Azure ML (our compute cluster there has 6 cores). Once the columns are imputed correctly, we'll zip the columns back together along with the original data for export. I'll also update our process to use the Azure key vault we set up instead of a local Databricks CLI."
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-26T08:13:30.564540100Z",
     "start_time": "2024-01-26T08:13:30.264113700Z"
    }
   },
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Imports and API setup\n",
    "import openai\n",
    "import tiktoken\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import json\n",
    "# import dotenv\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from io import StringIO\n",
    "import time\n",
    "\n",
    "# Use newer API version here\n",
    "#openai.api_key = dbutils.secrets.get(scope=\"openai\", key=\"access-token\")\n",
    "#openai.api_base = dbutils.secrets.get(scope=\"openai\", key=\"endpoint\") \n",
    "openai.api_type = 'azure'\n",
    "openai.api_version = '2023-12-01-preview'\n",
    "\n",
    "# Switch out deployments HERE too\n",
    "deployment_name='item-recommender-main' #This will correspond to the custom name you chose for your deployment when you deployed a model. "
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Temporary api key and base enviroment setting while I can't get the databricks CLI installed\n",
    "# DELETE THE STRINGS AFTER RUNNING THIS COMMAND\n",
    "openai.api_key = \"\"\n",
    "openai.api_base = \"\""
   ],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-26T08:13:31.866335900Z",
     "start_time": "2024-01-26T08:13:31.816982500Z"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'spark' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[4], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# Use the AWG data here since there's more control over where nulls can exist and the data's very similar to the UNFI data anyways\u001B[39;00m\n\u001B[1;32m----> 2\u001B[0m df_awg \u001B[38;5;241m=\u001B[39m \u001B[43mspark\u001B[49m\u001B[38;5;241m.\u001B[39mread\u001B[38;5;241m.\u001B[39mtable(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mawg_item_list\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m      4\u001B[0m \u001B[38;5;66;03m# Convert to Pandas dataframe\u001B[39;00m\n\u001B[0;32m      5\u001B[0m df_awg \u001B[38;5;241m=\u001B[39m df_awg\u001B[38;5;241m.\u001B[39mtoPandas()\n",
      "\u001B[1;31mNameError\u001B[0m: name 'spark' is not defined"
     ]
    }
   ],
   "source": [
    "# Use the AWG data here since there's more control over where nulls can exist and the data's very similar to the UNFI data anyways\n",
    "df_awg = spark.read.table(\"awg_item_list\")\n",
    "\n",
    "# Convert to Pandas dataframe\n",
    "df_awg = df_awg.toPandas()\n",
    "\n",
    "# Create artifical null values - the number of nulls is higher here than in the UNFI dataset to stress test the model a bit\n",
    "# Make sure the nulls only occur in the ADV columns though!\n",
    "# Define the columns where you want to introduce NaNs and the source columns for the imputer to read \n",
    "target_columns = ['ADV_Brand', 'ADV_Category', 'ADV_SubCategory', 'ADV_ItemDescrip', 'ADV_ItemUPC', 'ADV_CaseUPC10', 'ADV_Size', 'ADV_StorePack', 'RptLvlFilter'] # Need to make these read from the frontend in the future to be more programmatic\n",
    "source_columns = [\"Item Code\", \"Brand\", \"Category Name\", \"Sub Category Name\", \"UPCItem\", \"UPCCase\", \"Item Description\", \"Size\", \"Store Pack\"] \n",
    "\n",
    "# Define the proportion of NaNs you want in your DataFrame  \n",
    "nan_proportion = 0.4  # 40% NaNs  \n",
    "  \n",
    "# Calculate the number of values to replace with NaN for each column  \n",
    "nan_count_per_column = {column: int(df_awg.shape[0] * nan_proportion) for column in target_columns}  \n",
    "  \n",
    "# Randomly choose indices to be replaced with NaN for each target column\n",
    "np.random.seed(42) # Reproducibility\n",
    "for column in target_columns:  \n",
    "    nan_indices = np.random.choice(df_awg.index, nan_count_per_column[column], replace=False)  \n",
    "    df_awg.loc[nan_indices, column] = np.nan  \n",
    "\n",
    "# Separate this table into one free of null values and one containing nulls\n",
    "df_awg_test = df_awg[~(df_awg.notna().all(axis=1))]\n",
    "\n",
    "df_awg_train = df_awg[df_awg.notna().all(axis=1)]"
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Loop through the dataframe column-wise and call the model to impute nulls in the ADV_ columns\n",
    "for n, column in enumerate(target_columns):\n",
    "  # Basic prompt setup - plug in column names from here from frontend\n",
    "  prompt = f\"\"\"\n",
    "    The following data is a column of a dataset of product attributes called the AWG Item List: {df_awg_train.to_string(index=False, columns=[column], justify='center')} \n",
    "    The missing item attribution values need to be filled in. Here's how missing values in the ADV_ column are imputed manually:\n",
    "    the {target_columns[n]} column is based on the {source_columns[n]} column. \n",
    "\n",
    "    Autocomplete the data in the {target_columns[n]} column of the following validation sample from the AWG Item List:\n",
    "    {df_awg_test.to_string(index=False, columns=[column], justify='center')} and format your answer into a table.\n",
    "  \"\"\" \n",
    "  # While loop to prevent errors if the API doesn't connect successfully\n",
    "  retries = 3    \n",
    "  while retries > 0:    \n",
    "    try: \n",
    "      print(\"Imputing...\")\n",
    "      # API call to OpenAI GPT-4 deployment\n",
    "      response = openai.ChatCompletion.create(\n",
    "        engine=deployment_name,\n",
    "        temperature=1,\n",
    "        messages=[\n",
    "          {\"role\": \"system\", \"content\": \"You are a model used to fill in missing data.\"},\n",
    "          {\"role\": \"user\", \"content\": prompt}\n",
    "          ]\n",
    "      )\n",
    "      data_out = response['choices'][0]['message']['content']\n",
    "      print(f\"Imputed in column {target_columns[n]}\")\n",
    "      time.sleep(2)\n",
    "      break # End the while loop after it succeeds in calling the API. Might be bad practice to do things this way\n",
    "    except Exception as e:\n",
    "        print(e)  \n",
    "        retries -= 1  \n",
    "        if retries > 0:  \n",
    "            print('Timeout error, retrying...')  \n",
    "            time.sleep(5)  \n",
    "    else:  \n",
    "        print('API is not responding, all retries exhausted. Raising exception...')\n",
    "        raise ValueError(\"Time out error - try restarting the script and checking your connection to OpenAI Studio\")\n",
    "  \n",
    "  # Turn the output into a Pandas series\n",
    "  rows = data_out.split('\\n')\n",
    "  data_out_series = pd.Series(rows[1:], name=column)\n",
    "\n",
    "  # Append the series back to the test df\n",
    "  df_awg_test[column] = data_out_series\n"
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Zip up the test data back with the training data now that its filled out\n",
    "display(df_awg_test.head(100))"
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "source": [
    "### Some fixes to the above code\n"
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "source": [
    "%md\n",
    "# Item Attribution Recommender Project Testing\n",
    "\n",
    "Test out the capabilities of the API on Azure OpenAI Studio\n",
    "\n",
    "Here's some resources:\n",
    "- [Video on using Python with the studio](https://www.youtube.com/watch?v=gNCk8tW5QS8)\n",
    "- [Azure Documentation](https://learn.microsoft.com/en-us/azure/ai-services/openai/tutorials/embeddings?tabs=command-line)\n",
    "- [Medium article tutorial](https://medium.com/microsoftazure/querying-structured-data-with-azure-openai-e59ee43867e5)\n",
    "- [OpenAI API Documentation on Fine-Tuning](https://platform.openai.com/docs/guides/fine-tuning)\n",
    "- [Microsoft documentation on Bringing Your Own Data in Python](https://learn.microsoft.com/en-us/azure/ai-services/openai/use-your-data-quickstart?tabs=command-line&pivots=programming-language-python#create-the-python-app)\n",
    "- [Azure SDK Community Standup 9/14/23](https://www.youtube.com/watch?v=hFBXtGqTo7A)\n",
    "- [Azure OpenAI REST API Documentation](https://learn.microsoft.com/en-us/azure/ai-services/openai/reference)\n",
    "# Imports and API setup\n",
    "import openai\n",
    "import tiktoken\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import json\n",
    "# import dotenv\n",
    "# from azure.identity import DefaultAzureCredential\n",
    "\n",
    "# May need to update API version and deployment for the custom data section\n",
    "openai.api_key = dbutils.secrets.get(scope=\"openai\", key=\"access-token\") # Added using the Databricks CLI, see below for details\n",
    "openai.api_base = dbutils.secrets.get(scope=\"openai\", key=\"endpoint\")\n",
    "openai.api_type = 'azure'\n",
    "openai.api_version = '2023-05-15'\n",
    "\n",
    "# Switch out deployments HERE\n",
    "deployment_name='gpt-4-item-recommender' #This will correspond to the custom name you chose for your deployment when you deployed a model. \n",
    "%md\n",
    "To create/utilize secrets, you'll need to use the Databricks CLI. [Here's](https://learn.microsoft.com/en-us/azure/databricks/dev-tools/cli/install) how to install the CLI on your local machine (I directly used a ZIP of the CLI exe). Once the CLI is installed, you can create an access token in your Databricks user profile in the top right corner of the workspace. Add this profile in the CLI by following this [page](https://learn.microsoft.com/en-us/azure/databricks/dev-tools/cli/authentication).\n",
    "\n",
    "Once all that's set up, you can use the help command on the `databricks secrets put-secrets` and `databricks secrets create-scope` commands to add secrets for use in your notebooks. Don't follow the Microsoft syntax for these commands, as they use a different version of the CLI.\n",
    "# Send a completion call to generate an answer\n",
    "# Make sure to use the 2023-05-15 version of the API for this cell\n",
    "\n",
    "#start_phrase = 'Python can be used '\n",
    "#response = openai.Completion.create(engine=deployment_name, prompt=start_phrase, max_tokens=100)\n",
    "#text = response['choices'][0]['text'].replace('\\n', '').replace(' .', '.').strip()\n",
    "#print(start_phrase+text)\n",
    "# To add the unfi data, use the Azure Cognitive Search Index \"unfi-sample\" instead of the data directly in Databricks\n",
    "# Set up utility functions\n",
    "# These utility functions came directly from the Azure SDK Community Standup from 9/14/23, see above for link\n",
    "#openai.api_type = 'azure'\n",
    "#openai.api_version = '2023-08-01-preview' #Following a tutorial in terms of versioning - switch this on in case you use this cell\n",
    "\n",
    "def setup_byod(deployment_id: str) -> None:\n",
    "    \"\"\" Sets up the OpenAI Python SDK to use your own data for the chat endpoint \n",
    "\n",
    "       :param deployment_id: The deployment ID for the model to use with your own data.\n",
    "\n",
    "       To remove this configuration, simply set openai.requestssession to None.\n",
    "    \"\"\"\n",
    "    class BringYourOwnDataAdapter(requests.adapters.HTTPAdapter):\n",
    "\n",
    "        def send(self, request, **kwargs):\n",
    "            request.url = f\"{openai.api_base}/openai/deployments/{deployment_id}/extensions/chat/completions?api-version={openai.api_version}\"\n",
    "            return super().send(request, **kwargs)\n",
    "\n",
    "    session = requests.Session()\n",
    "    session.mount(\n",
    "        prefix=f\"{openai.api_base}/openai/deployments/{deployment_id}\",\n",
    "        adapter=BringYourOwnDataAdapter()\n",
    "    )\n",
    "\n",
    "    openai.requestssession = session\n",
    "# SDK Configuration\n",
    "# setup_byod(deployment_name)\n",
    "\n",
    "# Set up the chat functionality\n",
    "#def ask_question(question: str, show_context: bool = False) -> str:\n",
    "#completion = openai.ChatCompletion.create(\n",
    "#   messages=[{\"role\": \"user\", \"content\": question}],\n",
    "#  temperature=0.1,  # Keep temp low for deterministic answers\n",
    "# deployment_id=deployment_name,\n",
    "#dataSources=[\n",
    "#   {\n",
    "#      \"type\": \"AzureCognitiveSearch\",\n",
    "#     \"parameters\": {\n",
    "#        \"endpoint\": dbutils.secrets.get(\n",
    "#           scope=\"openai\", key=\"search-endpoint\"\n",
    "#      ),\n",
    "#     \"key\": dbutils.secrets.get(scope=\"openai\", key=\"search-key\"),\n",
    "#    \"indexName\": \"unfi-sample\",\n",
    "#},\n",
    "#}\n",
    "#],\n",
    "#)\n",
    "#if show_context:\n",
    "#    print(\"Context:\")\n",
    "#    print(\n",
    "#        json.dumps(\n",
    "#           json.loads(completion.choices[0].message.context.messages[0].content),\n",
    "#          indent=2,\n",
    "#     )\n",
    "# )\n",
    "#return f\"Answer: {completion.choices[0].message.content}\"\n",
    "%md\n",
    "The example above is working for simple queries of a sample of the dataset in txt format, but not for the pattern completion we need of this model. Instead of continuing with the BYOD chat completion, let's instead perform some prompt engineering and [few shot prompting](https://www.promptingguide.ai/techniques/fewshot) to try and get the model to recognize the pattern.\n",
    "# Bring in data from warehouse\n",
    "df_unfi = spark.read.table(\"unfi_item_list\")\n",
    "\n",
    "# Separate this table into one free of null values and one containing nulls\n",
    "df_unfi_train = df_unfi.filter(df_unfi[\"ADV_Category\"] != \"NA\")\n",
    "\n",
    "df_unfi_test = df_unfi.filter(df_unfi[\"ADV_Category\"] == \"NA\")\n",
    "# Convert a sample of the above dataframe to a str\n",
    "unfi_sample = str(df_unfi_train.head(45))\n",
    "\n",
    "# Create a \"validation\" set to test the AI's ability to recognize the pattern from the first sample\n",
    "unfi_sample_val = str(df_unfi_test.select(\"SVItemCD\", \"SVBrand\", \"SVDescrip\", \"VendPack\", \"SVSize\", \"SVPack\", \"UPCItem\", \"UPCCase\").tail(5))\n",
    "# Call the same model via the chat completion API\n",
    "# First, write out the prompt input\n",
    "#setup_byod(deployment_name)\n",
    "\n",
    "prompt = f\"\"\"\n",
    "The following data is a sample of a dataset of product attributes called the UNFI Item List. {unfi_sample} \n",
    "The first eight columns are used to fill in the last 9 columns. Can you autocomplete the last 9 columns of the following sample from the UNFI Item List:\n",
    "{unfi_sample_val} and format your answer into a table?\n",
    "\"\"\"\n",
    "\n",
    "response = openai.ChatCompletion.create(\n",
    "    engine=deployment_name,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a model used to fill in missing data.\"}, # Try out different language to avoid imputation tactics on the part of the model\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    "\n",
    ")\n",
    "\n",
    "print(f\"Answer: {response['choices'][0]['message']['content']}\")\n",
    "# Validation set for the above query\n",
    "display(df_unfi_test.tail(5))\n",
    "%md\n",
    "Results from this are better than above - the model is actually recognizing the data and attempting to impute nulls. However, the results are inconsistent in their quality, so we'll have to keep experimenting with prompt engineering as that's been the most successful method so far.\n",
    "# Try out splitting the task into sub-prompts\n",
    "# Imputation of Category and SubCategory\n",
    "prompt = f\"\"\"\n",
    "The following data is a sample of a dataset of product attributes called the UNFI Item List. {unfi_sample} \n",
    "The first eight columns are used to fill in the last 9 columns. Can you fill in the null values in the ADV_Category and ADV_SubCategory columns of the following sample from the UNFI Item List:\n",
    "{unfi_sample_val} and format your answer into a table? The ADV_Category column is based on the SVDescrip column and the ADV_SubCategory column is based on the SVBrand column.\n",
    "\"\"\"\n",
    "\n",
    "response = openai.ChatCompletion.create(\n",
    "    engine=deployment_name,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a model used to fill in missing data.\"}, # Try out different language to avoid imputation tactics on the part of the model\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    "\n",
    ")\n",
    "\n",
    "print(f\"Answer: {response['choices'][0]['message']['content']}\")\n",
    "# Imputation of the rest of the columns\n",
    "prompt = f\"\"\"\n",
    "The following data is a sample of a dataset of product attributes called the UNFI Item List. {unfi_sample} \n",
    "The first eight columns are used to fill in the last 7 columns. Can you autocomplete the last 7 columns of the following sample from the UNFI Item List:\n",
    "{unfi_sample_val} and format your answer into a table?\n",
    "\"\"\"\n",
    "\n",
    "response = openai.ChatCompletion.create(\n",
    "    engine=deployment_name,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a model used to fill in missing data.\"}, # Try out different language to avoid imputation tactics on the part of the model\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    "\n",
    ")\n",
    "\n",
    "print(f\"Answer: {response['choices'][0]['message']['content']}\")\n",
    "# Validation set for the above queries\n",
    "display(df_unfi_test.tail(5))\n",
    "display(unfi_sample_val)\n",
    "%md\n",
    "## Testing API Changes/2nd Code Draft\n",
    "\n",
    "The code below is a more refined and updated version of the stuff above. I'll keep all of the preceding code for posterity's sake, but know that the code below is closer to the final version.\n",
    "# New data import - use Pandas instead\n",
    "# Bring in data from warehouse\n",
    "df_unfi = spark.read.table(\"unfi_item_list\")\n",
    "\n",
    "# Convert to Pandas dataframe\n",
    "df_unfi = df_unfi.toPandas()\n",
    "\n",
    "# Replace na with actual null values\n",
    "df_unfi = df_unfi.replace('NA', np.nan)\n",
    "#df_unfi.isnull().sum()\n",
    "\n",
    "# Separate this table into one free of null values and one containing nulls\n",
    "df_unfi_test = df_unfi[~(df_unfi.notna().all(axis=1))]\n",
    "\n",
    "df_unfi_train = df_unfi[df_unfi.notna().all(axis=1)]\n",
    "\n",
    "# Convert a sample of the above dataframe to a str\n",
    "unfi_sample = df_unfi_train.head(800).to_string(sparsify=False, justify=\"center\") # Keep this at 800 max while using gpt-4 turbo\n",
    "\n",
    "# Create a \"validation\" set to test the AI's ability to recognize the pattern from the first sample\n",
    "unfi_sample_val = df_unfi_test.head(20).to_string(sparsify=False, justify=\"center\")\n",
    "%md\n",
    "Between the timeout errors and the model running out of output tokens, it looks like the limit for output is **15 rows at a time**. This means we'll have to implement a loop, maybe we can parallelize the task so we can run multiple loops at once. Trying to get outputs beyond 15 rows makes the run time increase exponentially - it goes from a few minutes to over an hour. This might be because trying to output over the token limit makes the model hang until it times out. So maybe another solution here is to get a quota increase on the output tokens like we did with the input tokens.\n",
    "# Call the same model via the chat completion API\n",
    "# Also add more detail in the prompt on what columns map to each other\n",
    "# To-do: Create a user defined method of mapping columns\n",
    "prompt = f\"\"\"\n",
    "The following data is a sample of a dataset of product attributes called the UNFI Item List: {unfi_sample} \n",
    "The missing item attribution values need to be filled in. Here's how the columns map to each other:\n",
    "the ADV_Category column is based on the SVItemCD column,\n",
    "the ADV_Brand column is based on the SVBrand column,\n",
    "the ADV_SubCategory column is inferred using the SVBItemCD and SVItemDescrip columns,\n",
    "the ADV_Brand column is based on the SVBrand column,\n",
    "the ADV_ItemDescrip column is based on the SVDescrip column,\n",
    "the ADV_Size column is based on the SVSize column,\n",
    "the ADV_Store_Pack cloumn is based on the VendPack column,\n",
    "the ADV_ItemUPC column is copied from the UPCItem column and should not be formatted in scientific notation,\n",
    "the ADV_CaseUPC10 column is exactly the same as the ADV_ItemUPC column and should not be formatted in scientific notation,\n",
    "and the RptLvlFilter column values can be left null.\n",
    "\n",
    "Autocomplete the last 9 columns of the following validation sample from the UNFI Item List:\n",
    "{unfi_sample_val} and format your answer into a table using | to separate columns. Make sure your output starts with '| SVItemCD |' and ends with '| EXCLUDE      |'. Do not insert a blank row under the header in the output. \n",
    "\"\"\"\n",
    "##### I received a few responses that some variables were undefined, so I defined them earlier. Then I received this\n",
    "##### this message for the response object: \n",
    "# To fix the error, you need to reduce the length of the messages sent to the OpenAI API. The error states that the maximum context length is 8192 tokens, but your messages resulted in 79184 tokens. Here are a few suggestions to reduce the length:\n",
    "\n",
    "# Remove unnecessary details from the prompt.\n",
    "\n",
    "# Go through the prompt and remove any extra information that is not essential for generating the desired output.\n",
    "# Focus on providing the required information and instructions concisely.\n",
    "# Split the prompt into multiple parts or messages.\n",
    "\n",
    "# Instead of sending the entire prompt as a single message, split it into smaller parts if possible.\n",
    "# Send the parts sequentially as separate messages to stay within the token limit.\n",
    "# Reduce the length of variable values.\n",
    "\n",
    "# If the variables unfi_sample and unfi_sample_val contain long strings, try to shorten them or use a summarization technique.\n",
    "# Use shorter or more concise language.\n",
    "\n",
    "# Review the content of the prompt and messages, and see if you can express the same information using fewer words or shorter sentences.\n",
    "\n",
    "response = openai.ChatCompletion.create(\n",
    "    engine=deployment_name,\n",
    "    request_timeout=3600, # Up the time to timeout significantly - this is in seconds\n",
    "    temperature=1, # Try raising this to suppress the \"I can't generate data\" response\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a model used to fill in missing data.\"}, # Try out different language to avoid imputation tactics on the part of the model\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    "\n",
    ")\n",
    "\n",
    "print(f\"Answer: {response['choices'][0]['message']['content']}\")\n",
    "%md\n",
    "If you need help with the Chat Completion API and Python function, [here](https://platform.openai.com/docs/api-reference/introduction?lang=python) is a reference from OpenAI and [here's](https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/chatgpt?tabs=python&pivots=programming-language-chat-completions) one from Microsoft. Here's a good [Stackoverflow thread](https://stackoverflow.com/questions/75787638/openai-gpt-3-api-error-request-timed-out) on the timeout issues and how to turn the above code into production code.\n",
    "# Take the response from the bot and transfer the data into an exportable format\n",
    "output = response['choices'][0]['message']['content']\n",
    "\n",
    "# Trim out the non-tabular data\n",
    "output_start = output.find('| SVItemCD |')\n",
    "output_end = output.rfind('| EXCLUDE    |')\n",
    "\n",
    "# StringIO converts the trimmed response string to a file pandas read_csv can convert to a dataframe\n",
    "output_df = pd.read_csv(StringIO(output[output_start:output_end]), delimiter='|', index_col=[0], skiprows=[1]) # Skip the first row where the model keeps putting dash marks\n",
    "\n",
    "display(output_df)\n",
    "# Validation set for the above query\n",
    "display(df_unfi_test.head(5))\n",
    "%md\n",
    "## Testing out the Model on the AWG dataset\n",
    "\n",
    "This dataset doesn't have any natural null values to impute, so we'll need to create nulls artifically using numpy. Source [here](https://cmdlinetips.com/2019/05/how-to-randomly-add-nan-to-pandas-dataframe/)\n",
    "# Bring in AWG data\n",
    "df_awg = spark.read.table(\"awg_item_list\")\n",
    "\n",
    "# Convert to Pandas dataframe\n",
    "df_awg = df_awg.toPandas()\n",
    "\n",
    "# Create artifical null values - the number of nulls is higher here than in the UNFI dataset to stress test the model a bit\n",
    "# Make sure the nulls only occur in the ADV columns though!\n",
    "# Define the columns where you want to introduce NaNs  \n",
    "target_columns = ['ADV_Brand', 'ADV_Category', 'ADV_SubCategory', 'ADV_ItemDescrip', 'ADV_ItemUPC', 'ADV_CaseUPC10', 'ADV_Size', 'ADV_StorePack', 'RptLvlFilter']\n",
    "\n",
    "# Define the proportion of NaNs you want in your DataFrame  \n",
    "nan_proportion = 0.4  # 40% NaNs  \n",
    "\n",
    "# Calculate the number of values to replace with NaN for each column  \n",
    "nan_count_per_column = {column: int(df_awg.shape[0] * nan_proportion) for column in target_columns}\n",
    "\n",
    "# Randomly choose indices to be replaced with NaN for each target column  \n",
    "for column in target_columns:\n",
    "nan_indices = np.random.choice(df_awg.index, nan_count_per_column[column], replace=False)\n",
    "df_awg.loc[nan_indices, column] = np.nan\n",
    "\n",
    "# Separate this table into one free of null values and one containing nulls\n",
    "df_awg_test = df_awg[~(df_awg.notna().all(axis=1))]\n",
    "\n",
    "df_awg_train = df_awg[df_awg.notna().all(axis=1)]\n",
    "\n",
    "# Convert a sample of the above dataframe to a str\n",
    "awg_sample = df_awg_train.head(800).to_string(sparsify=False, justify=\"center\") # Keep this at 200 while using the 32k gpt 4 model\n",
    "\n",
    "# Create a \"validation\" set to test the AI's ability to recognize the pattern from the first sample\n",
    "awg_sample_val = df_awg_test.head(5).to_string(sparsify=False, justify=\"center\")\n",
    "# Model prompt\n",
    "prompt = f\"\"\"\n",
    "The following data is a sample of a dataset of product attributes called the AWG Item List: {awg_sample} \n",
    "The missing item attribution values need to be filled in. Here's how the columns map to each other:\n",
    "the ADV_Category column is based on the Category Name column,\n",
    "the ADV_Brand column is based on the Brand column,\n",
    "the ADV_SubCategory column is inferred using the Sub Category Name column,\n",
    "the ADV_ItemDescrip column is based on the Item Description column,\n",
    "the ADV_Size column is based on the Size column,\n",
    "the ADV_Store_Pack cloumn is based on the Store Pack column,\n",
    "the ADV_ItemUPC column is copied from the UPCItem column and should not be formatted in scientific notation,\n",
    "the ADV_CaseUPC10 column is exactly the same as the UPCCase column and should not be formatted in scientific notation,\n",
    "and the RptLvlFilter column values can be left null.\n",
    "\n",
    "Autocomplete the last 9 columns of the following validation sample from the AWG Item List:\n",
    "{awg_sample_val} and format your answer into a table using | to separate columns. Make sure your output starts with '| Item Code |' and ends with '| INCLUDE      |'.\n",
    "\"\"\"\n",
    "\n",
    "response = openai.ChatCompletion.create(\n",
    "engine=deployment_name,\n",
    "temperature=1, # Try raising this to suppress the \"I can't generate data\" response\n",
    "messages=[\n",
    "{\"role\": \"system\", \"content\": \"You are a model used to fill in missing data.\"}, # Try out different language to avoid imputation tactics on the part of the model\n",
    "{\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "\n",
    ")\n",
    "\n",
    "print(f\"Answer: {response['choices'][0]['message']['content']}\")\n",
    "%md\n",
    "## Final Draft on Dev Environment\n",
    "\n",
    "After meeting with the team on 1/22/24, I'm trying a new approach to the imputation where the data will be split column-wise and fed into the model separately. This will hopefully avoid the token/timeout issues we've been experiencing lately and allow us to feed the model more input columns for training. It also makes sense from a parallelization standpoint; we can run 6 of these column API calls at the same time to save time on the overall processing in Azure ML (our compute cluster there has 6 cores). Once the columns are imputed correctly, we'll zip the columns back together along with the original data for export. I'll also update our process to use the Azure key vault we set up instead of a local Databricks CLI.\n",
    "# Temporary api key and base enviroment setting while I can't get the databricks CLI installed\n",
    "# DELETE THE STRINGS AFTER RUNNING THIS COMMAND\n",
    "openai.api_key = \"\"\n",
    "openai.api_base = \"\"\n",
    "# I commented out some packaged installs that did not seem to be used\n",
    "# and were problematic to install\n",
    "\n",
    "# Imports and API setup\n",
    "import openai\n",
    "import tiktoken\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import json\n",
    "# import dotenv\n",
    "# from azure.identity import DefaultAzureCredential\n",
    "from io import StringIO\n",
    "import time\n",
    "\n",
    "# Use newer API version here\n",
    "#openai.api_key = dbutils.secrets.get(scope=\"openai\", key=\"access-token\")\n",
    "#openai.api_base = dbutils.secrets.get(scope=\"openai\", key=\"endpoint\") \n",
    "openai.api_type = 'azure'\n",
    "openai.api_version = '2023-12-01-preview'\n",
    "\n",
    "# Switch out deployments HERE too\n",
    "deployment_name='item-recommender-main' #This will correspond to the custom name you chose for your deployment when you deployed a model. \n",
    "# Use the AWG data here since there's more control over where nulls can exist and the data's very similar to the UNFI data anyways\n",
    "df_awg = spark.read.table(\"awg_item_list\")\n",
    "\n",
    "# Convert to Pandas dataframe\n",
    "df_awg = df_awg.toPandas()\n",
    "\n",
    "# Create artifical null values - the number of nulls is higher here than in the UNFI dataset to stress test the model a bit\n",
    "# Make sure the nulls only occur in the ADV columns though!\n",
    "# Define the columns where you want to introduce NaNs and the source columns for the imputer to read \n",
    "target_columns = ['ADV_Brand', 'ADV_Category', 'ADV_SubCategory', 'ADV_ItemDescrip', 'ADV_ItemUPC', 'ADV_CaseUPC10', 'ADV_Size', 'ADV_StorePack', 'RptLvlFilter'] # Need to make these read from the frontend in the future to be more programmatic\n",
    "source_columns = [\"Item Code\", \"Brand\", \"Category Name\", \"Sub Category Name\", \"UPCItem\", \"UPCCase\", \"Item Description\", \"Size\", \"Store Pack\"]\n",
    "\n",
    "# Define the proportion of NaNs you want in your DataFrame  \n",
    "nan_proportion = 0.4  # 40% NaNs  \n",
    "\n",
    "# Calculate the number of values to replace with NaN for each column  \n",
    "nan_count_per_column = {column: int(df_awg.shape[0] * nan_proportion) for column in target_columns}\n",
    "\n",
    "# Randomly choose indices to be replaced with NaN for each target column\n",
    "np.random.seed(42) # Reproducibility\n",
    "for column in target_columns:\n",
    "nan_indices = np.random.choice(df_awg.index, nan_count_per_column[column], replace=False)\n",
    "df_awg.loc[nan_indices, column] = np.nan\n",
    "\n",
    "# Separate this table into one free of null values and one containing nulls\n",
    "df_awg_test = df_awg[~(df_awg.notna().all(axis=1))]\n",
    "\n",
    "df_awg_train = df_awg[df_awg.notna().all(axis=1)]\n",
    "# Loop through the dataframe column-wise and call the model to impute nulls in the ADV_ columns\n",
    "for n, column in enumerate(target_columns):\n",
    "# Basic prompt setup - plug in column names from here from frontend\n",
    "prompt = f\"\"\"\n",
    "    The following data is a column of a dataset of product attributes called the AWG Item List: {df_awg_train.to_string(index=False, columns=[column], justify='center')} \n",
    "    The missing item attribution values need to be filled in. Here's how missing values in the ADV_ column are imputed manually:\n",
    "    the {target_columns[n]} column is based on the {source_columns[n]} column. \n",
    "\n",
    "    Autocomplete the data in the {target_columns[n]} column of the following validation sample from the AWG Item List:\n",
    "    {df_awg_test.to_string(index=False, columns=[column], justify='center')} and format your answer into a table.\n",
    "  \"\"\"\n",
    "# While loop to prevent errors if the API doesn't connect successfully\n",
    "retries = 3\n",
    "while retries > 0:\n",
    "    ### moved this definition up\n",
    "    data_out = response['choices'][0]['message']['content']\n",
    "    print(f\"Imputed in column {target_columns[n]}\")\n",
    "    time.sleep(2)\n",
    "    try:\n",
    "        print(\"Imputing...\")\n",
    "        # API call to OpenAI GPT-4 deployment\n",
    "        response = openai.ChatCompletion.create(\n",
    "            engine=deployment_name,\n",
    "            temperature=1,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a model used to fill in missing data.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ]\n",
    "        )\n",
    "        data_out = response['choices'][0]['message']['content']\n",
    "        print(f\"Imputed in column {target_columns[n]}\")\n",
    "        time.sleep(2)\n",
    "        break # End the while loop after it succeeds in calling the API. Might be bad practice to do things this way\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        retries -= 1\n",
    "        if retries > 0:\n",
    "            print('Timeout error, retrying...')\n",
    "            time.sleep(5)\n",
    "    else:\n",
    "        print('API is not responding, all retries exhausted. Raising exception...')\n",
    "        raise ValueError(\"Time out error - try restarting the script and checking your connection to OpenAI Studio\")\n",
    "\n",
    "    # Turn the output into a Pandas series\n",
    "rows = data_out.split(\"'\\n'\")\n",
    "data_out_series = pd.Series(rows[1:], name=column)\n",
    "\n",
    "# Append the series back to the test df\n",
    "df_awg_test[column] = data_out_series\n",
    "\n",
    "# Zip up the test data back with the training data now that its filled out\n",
    "display(df_awg_test.head(100))"
   ],
   "execution_count": null
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Item Attribution Project Testing",
   "widgets": {}
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
