{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b02b64ee-ab8b-45d4-8d40-d6aa7ab4efbf",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Evaluating Uplift Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33dd3061-4463-48d8-85a5-40d848707f22",
   "metadata": {},
   "source": [
    "One of the most widespread applications of causal inference in the industry is **uplift modeling**, a.k.a. the estimation of Conditional Average Treatment Effects.\n",
    "\n",
    "When estimating the causal effect of a **treatment** (a drug, ad, product, ...) on an **outcome** of interest (a disease, firm revenue, customer satisfaction, ...), we are often not only interested in understanding whether the treatment works on average, but we would like to know for which **subjects** (patients, users, customers, ...) it works better or worse. \n",
    "\n",
    "Estimating heterogeneous incremental effects, or uplift, is an essential intermediate step to improve **targeting** of the policy of interest. For example, we might want to warn certain people that they are more likely to experience side effects from a drug or show an advertisement only to a specific set of customers.\n",
    "\n",
    "While there exist many methods to model uplift, it is not always clear which one to use in a specific application. Crucially, because of the **fundamental problem of causal inference**, the objective of interest, the uplift, is never observed, and therefore we cannot validate our estimators as we would do with a machine learning prediction algorithm. We cannot set aside a validation set and pick the best-performing model since we have **no ground truth**, not even in the validation set, and not even if we ran a randomized experiment.\n",
    "\n",
    "What can we do then? In this article, I try to cover the most popular methods used to **evaluate uplift models**. If you are unfamiliar with uplift models, I suggest first reading my introductory article."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e518cd8c-94cd-4026-b577-fac2b618fa0b",
   "metadata": {},
   "source": [
    "https://towardsdatascience.com/8a9c1e340832"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a748a381-48ed-4ad4-8728-41311371caae",
   "metadata": {},
   "source": [
    "## Uplift and Promotional Emails"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b1d9d4-e9c1-4149-8a0b-49c5b4012ba5",
   "metadata": {},
   "source": [
    "Imagine we were working in the marketing department of a product company interested in improving our **email marketing campaign**. Historically, we mostly sent emails to new customers. However, now we would like to adopt a data-driven approach and target customers for whom the email has the highest positive impact on revenue. This impact is also called **uplift** or **incrementality**.\n",
    "\n",
    "Let's have a look at the data we have at our disposal. I import the data-generating process `dgp_promotional_email()` from [`src.dgp`](https://github.com/matteocourthoud/Blog-Posts/blob/main/notebooks/src/dgp.py). I also import some plotting functions and libraries from [`src.utils`](https://github.com/matteocourthoud/Blog-Posts/blob/main/notebooks/src/utils.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22cf8401-6ffa-4f22-b85e-65a9681e8c9f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d166a29-c389-40ec-b6d2-028bd9c3a7be",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from src.utils import *\n",
    "from src.dgp import dgp_promotional_email"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4579de5d-20f1-4696-b01d-e1db675118c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dgp = dgp_promotional_email(n=500)\n",
    "df = dgp.generate_data()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c8fc33b-3769-46eb-afdb-9708c2ed648a",
   "metadata": {},
   "source": [
    "We have information on 500 customers, for whom we observe whether they are `new` customers, their `age`, the sales they generated before the email campaign (`sales_old`), whether they were sent the `mail`, and the `sales` after the email campaign.\n",
    "\n",
    "The **outcome** of interest is `sales`, which we denote with the letter *Y*. The **treatment** or policy that we would like to improve is the `mail` campaign, which we denote with the letter *W*. We call all the remaining variables **confounders** or control variables and we denote them with *X*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "031fee37-6b4a-4750-9c2a-1d0d0aeacafe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Y = 'sales'\n",
    "W = 'mail'\n",
    "X = ['age', 'sales_old', 'new']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d26fbfc5-df09-4f2e-944a-08aec20dd83b",
   "metadata": {},
   "source": [
    "The Dyrected Acyclic Graph (DAG) representing the causal relationships between the variables is the following. The causal relationship of interest is depicted in green. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a27512c9-c75a-4627-baa1-a47379102cba",
   "metadata": {},
   "source": [
    "```mermaid\n",
    "flowchart TD\n",
    "classDef included fill:#DCDCDC,stroke:#000000,stroke-width:3px;\n",
    "\n",
    "W((mail))\n",
    "Y((sales))\n",
    "X1((new))\n",
    "X2((age))\n",
    "X3((sales old))\n",
    "\n",
    "W --> Y\n",
    "X1 --> W\n",
    "X1 --> Y\n",
    "X2 --> Y\n",
    "X3 --> Y\n",
    "\n",
    "class W,Y,X1,X2,X3 included;\n",
    "\n",
    "linkStyle 0 stroke:#2db88b,stroke-width:6px;\n",
    "linkStyle 1,2,3,4 stroke:#003f5c,stroke-width:6px;\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b380190-654f-4244-b525-0054ae5d58cd",
   "metadata": {},
   "source": [
    "From the DAG we see that the `new` customer indicator is a confounder and needs to be controlled for in order to identify the effect of `mail` on `sales.` `age` and `sales_old` instead are not essential for estimation but could be helpful for identification. For more information on DAGs and control variables, you can check my introductory article."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0923a59c-9a54-43e8-97dd-02862b9b6639",
   "metadata": {},
   "source": [
    "https://towardsdatascience.com/b63dc69e3d8c"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3850428-fcb8-455d-a58b-2f08761a514b",
   "metadata": {},
   "source": [
    "The objective of uplift modeling is to recover the **Individual Treatment Effects (ITE)** $\\tau_i$, i.e. the incremental effect on `sales` of sending the promotional `mail`. We can express the ITE as the difference between two hypothetical quantities: the potential outcome of the customer if they had received the email, $Y_i^{(1)}$, minus the potential outcome of the customer if they had *not* received the email, $Y_i^{(0)}$.\n",
    "$$\n",
    "\\tau_i = Y_i^{(1)} - Y_i^{(0)}\n",
    "$$\n",
    "\n",
    "Note that for each customer, we only observe one of the two realized outcomes, depending on whether they actually received the `mail` or not. Therefore, the ITE are inherently unobservable. What can be estimated instead is the **Conditional Average Treatment Effect (CATE)** i.e., the expected individual treatment effect $\\tau_i$, conditional on covariates *X*. For example, the average effect of the `mail` on `sales` for older customers (`age` > 50).\n",
    "$$\n",
    "\\tau(x) = \\mathbb{E} \\Big[ \\ \\tau_i \\ \\Big| \\ X_i = x \\Big]\n",
    "$$\n",
    "\n",
    "In order to be able to recover the CATE, we need to make three assumptions.\n",
    "\n",
    "1. **Unconfoundedness**: $Y^{(0)}, Y^{(1)} \\perp W \\ | \\ X$\n",
    "\n",
    "2. **Overlap**: $0 < e(X) < 1$\n",
    "\n",
    "3. **Consistency**: $Y = W \\cdot Y^{(1)} + (1-W) \\cdot Y^{(0)}$\n",
    "\n",
    "Where $e(X)$ is the **propensity score** i.e., the expected probability of being treated, conditional on covariates *X*.\n",
    "$$\n",
    "e(x) = \\mathbb{E} \\Big[ \\ W_i \\ \\Big| \\ X_i = x \\Big]\n",
    "$$\n",
    "\n",
    "\n",
    "In what follows, we will use machine learning methods to estimate the CATE $\\tau(x)$, the propensity scores $e(x)$, and the conditional expectation function (CEF) of the outcome, $\\mu(x)$\n",
    "$$\n",
    "\\mu(x) = \\mathbb{E} \\Big[ \\ Y_i \\ \\Big| \\ X_i = x \\Big]\n",
    "$$\n",
    "\n",
    "We use [Random Forest Regression](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html) algorithms to model the CATE and the outcome CEF, while we use [Logistic Regression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) to model the propensity score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "583ddc30-6747-42f5-8519-fc7a8d86dc3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "\n",
    "model_tau = RandomForestRegressor(max_depth=2)\n",
    "model_y = RandomForestRegressor(max_depth=2)\n",
    "model_e = LogisticRegressionCV()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbca655c-19fd-4764-9306-f83c4d747122",
   "metadata": {},
   "source": [
    "In this article, we do not fine-tune the underlying machine learning models, but fine-tuning is strongly recommended to improve the accuracy of uplift models (for example, with auto-ml libraries like [FLAML](https://microsoft.github.io/FLAML/))."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13b45196-7551-4562-a8b6-b3f7c1d0a905",
   "metadata": {},
   "source": [
    "## Uplift Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f9d8ffe-523b-4b6c-ae71-30c6ace00ade",
   "metadata": {},
   "source": [
    "There exist **many methods** to model uplift or, in other words, to estimate Conditional Average Treatment Effects (CATE). Since the objective of this article is to compare methods to *evaluate* uplift models, we will not explain the methods in detail. For a gentle introduction, you can check [my introductory article on meta learners](https://medium.com/towards-data-science/understanding-meta-learners-8a9c1e340832).\n",
    "\n",
    "The learners that we will consider are the following:\n",
    "\n",
    "- S-learner or single-learner, introduced by [Kunzel, Sekhon, Bickel, Yu (2017)](https://arxiv.org/abs/1706.03461)\n",
    "\n",
    "- T-learner or two-learner, introduced by [Kunzel, Sekhon, Bickel, Yu (2017)](https://arxiv.org/abs/1706.03461)\n",
    "\n",
    "- X-learner or cross-learner, introduced by [Kunzel, Sekhon, Bickel, Yu (2017)](https://arxiv.org/abs/1706.03461)\n",
    "\n",
    "- R-learner or [Robinson](https://www.jstor.org/stable/1912705)-learner introduced by [Nie, Wager (2017)](https://arxiv.org/abs/1712.04912)\n",
    "\n",
    "- DR-learner or doubly-robust-learner, introduced by [Kennedy (2022)](https://arxiv.org/abs/2004.14497)\n",
    "\n",
    "\n",
    "We import all the model from Microsoft's [econml](https://econml.azurewebsites.net/) library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47294102-7b75-4a52-afdd-3b364111ee1a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from src.learners_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8404052-bca4-48e7-8414-3b5e33d7ae5a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from econml.metalearners import SLearner, TLearner, XLearner\n",
    "from econml.dml import NonParamDML\n",
    "from econml.dr import DRLearner\n",
    "\n",
    "S_learner = SLearner(overall_model=model_y)\n",
    "T_learner = TLearner(models=clone(model_y))\n",
    "X_learner = XLearner(models=model_y, propensity_model=model_e, cate_models=model_tau)\n",
    "R_learner = NonParamDML(model_y=model_y, model_t=model_e, model_final=model_tau, discrete_treatment=True)\n",
    "DR_learner = DRLearner(model_regression=model_y, model_propensity=model_e, model_final=model_tau)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c28dfab-7d5f-462c-9d3c-95e29517635e",
   "metadata": {},
   "source": [
    "We `fit()` the models on the data, specifying the outcome variable *Y*, the treatment variable *W* and covariates *X*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "938fb5fb-c8cf-41a0-a4b5-7d72fd287546",
   "metadata": {},
   "outputs": [],
   "source": [
    "names = ['SL', 'TL', 'XL', 'RL', 'DRL']\n",
    "learners = [S_learner, T_learner, X_learner, R_learner, DR_learner]\n",
    "for learner in learners:\n",
    "    learner.fit(df[Y], df[W], X=df[X])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "829a8262-f382-4067-b1bb-3d82a45ecb42",
   "metadata": {},
   "source": [
    "We are now ready to evaluate the models! Which model should we choose?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d334ebd-bb27-40d6-9459-897c3226d859",
   "metadata": {},
   "source": [
    "## Oracle Loss Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d8369eb-b715-4a6a-aabe-3c245b2110bc",
   "metadata": {},
   "source": [
    "The main problem of evaluating uplift models is that, even with a validation set and even with a randomized experiment or AB test, we do **not observe** our metric of interest: the Individual Treatment Effects. In fact, we only observe the realized outcomes, $Y_i^{(0)}$ for untreated customers and $Y_i^{(1)}$ for treated customers. Therefore, for no customer we can compute the individual treatment effect in the validation data, $\\tau_i = Y_i^{(1)} - Y_i^{(0)}$.\n",
    "\n",
    "Can we still do something to **evaluate** our estimators?\n",
    "\n",
    "The answer is yes, but before giving more details, let's first understand what we would do if we **could observe** the Individual Treatment Effects $\\tau_i$. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16a9b3f7-1283-443d-8cb5-e3a082a065ef",
   "metadata": {},
   "source": [
    "### Oracle MSE Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef7dc846-795d-4e6e-b48f-ad9e09c36b43",
   "metadata": {},
   "source": [
    "If we could observe the individual treatment effects (but we don't, hence the \"oracle\" attribute), we could try to measure how far our estimates $\\hat{\\tau}(X_i)$ are from the true values $\\tau_i$. This is what we normally do in machine learning when we want to evaluate a prediction method: we set aside a validation dataset and we compare predicted and true values on that data. There exist plenty of loss functions to evaluate prediction accurary, so let's concentrate on the most popular one: the **Mean Squared Error (MSE) loss**.\n",
    "\n",
    "$$\n",
    "\\mathcal{L} _ {oracle-MSE}(\\hat{\\tau}) = \\frac{1}{n} \\sum _ {i=1}^{n} \\left( \\hat{\\tau}(X_i) - \\tau(X_i) \\right)^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9583e38d-f840-4cfb-8ca9-4e8c08320963",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def loss_oracle_mse(data, learner):\n",
    "    tau = learner.effect(data[X])\n",
    "    return np.mean((tau - data['effect_on_sales'])**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66e343ee-5412-41b1-9c02-4fad8b3113af",
   "metadata": {},
   "source": [
    "The function `compare_methods` prints and plots evaluation metrics computed on a separate validation dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e53adbc9-f2c0-44c1-9803-01af208cb873",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_methods(learners, names, loss, title=None, subtitle='lower is better'):\n",
    "    data = dgp.generate_data(seed_data=1, seed_assignment=1, keep_po=True)\n",
    "    results = pd.DataFrame({\n",
    "        'learner': names,\n",
    "        'loss': [loss(data.copy(), learner) for learner in learners]\n",
    "    })\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(6, 4))\n",
    "    sns.barplot(data=results, x=\"learner\", y='loss').set(ylabel='')\n",
    "    plt.suptitle(title, y=1.02)\n",
    "    plt.title(subtitle, fontsize=12, fontweight=None, y=0.94)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1017f084-a1ad-4545-b222-a08d332be64c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "compare_methods(learners, names, loss_oracle_mse, title='Oracle MSE Loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "653a2e44-027e-4925-8747-ebf8d702e5c7",
   "metadata": {},
   "source": [
    "In this case, we see that the T-learner clearly performs worst, with the S-learner just behind. On the other hand, the X-, R- and DR-learners perform significantly better, with the **DR-learner winning** the race.\n",
    "\n",
    "However, this might *not* be the best loss function to evaluate our uplift model. In fact, uplift modeling is just an intermediate step towards our ultimate goal: improving revenue."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51a252fd-241d-46fd-a949-5490f3b6842f",
   "metadata": {},
   "source": [
    "### Oracle Policy Gain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d39b2c02-e18d-47c7-918e-e246ee2066b0",
   "metadata": {},
   "source": [
    "Since our ultimate goal is to **improve revenue**, we could evaluate estimators by how much they increase revenue, given a certain policy function. Suppose, for example, that we had a $0.01$\\\\$ cost of sending an email. Then, our policy would be to treat each costumer that has a predicted Conditional Average Treatment Effect above $0.01$\\\\$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdedc18d-0bee-4721-8e29-23e459119101",
   "metadata": {},
   "outputs": [],
   "source": [
    "cost = 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b8e734c-7789-410e-aafb-eeab9d4459dd",
   "metadata": {},
   "source": [
    "How much would our revenue actually increase? Let's define with $d(\\hat{\\tau})$ our policy function, such that $d=1$ if $\\tau >= 0.1$ and $d=0$ otherwise. Then our *gain* (higher is better) function is:\n",
    "$$\n",
    "\\mathcal{G} _ {oracle-POLICY}(\\hat{\\tau}) = \\frac{1}{n} \\sum _ {i=1}^{n} d(\\hat{\\tau}) (\\tau_i - c)\n",
    "$$\n",
    "\n",
    "Again, this is an \"oracle\" loss function that **cannot be computed** in reality since we do not observe the individual treatment effects. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b34fc7-0cc1-4d1d-9a5c-2b568b9b9bd0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def gain_oracle_policy(data, learner):\n",
    "    tau_hat = learner.effect(data[X])\n",
    "    return np.sum((data['effect_on_sales'] - cost) * (tau_hat > cost))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c830407e-2bbd-46ce-817e-c783c96cb222",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "compare_methods(learners, names, gain_oracle_policy, title='Oracle Policy Gain', subtitle='higher is better')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86a92175-dca7-482e-8507-3fc7f02143ef",
   "metadata": {},
   "source": [
    "In this case, the S-learner is clearly the worst performer, leading to no effect on revenues. The T-learner leads to modest gains while the X-, R- and DR- learners all lead to aggregate gains, with the **X-learner slightly ahead**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d29ad403-c652-4b0f-86cc-51099e168bb0",
   "metadata": {},
   "source": [
    "## Practical Loss Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45499d71-0802-457d-8b11-4f34c124af51",
   "metadata": {},
   "source": [
    "In the previous section, we have seen two examples of loss functions that we would like to compute if we could observe the Individual Treatment Effects $\\tau_i$. However, in practice, even with a randomized experiment and even with a validation set, we do not observe the ITE,our object of interest. We will now cover some measures that try to evaluate uplift models, given this practical constraint."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4c02b29-a4b5-4ab2-8a52-0a343427f2a8",
   "metadata": {},
   "source": [
    "### Outcome Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64492e0e-cfab-43b5-8a26-ee2dea775617",
   "metadata": {},
   "source": [
    "The first and simplest approach is to switch to a different loss variable. While we cannot observe the Individual Treatment Effects, $\\tau_i$, we can still observe our outcome $y_i$. This is not exactly our object of interest, but we might expect an uplift model that performs well in terms of predicting $y$ to also produce good estimates of $\\tau$.\n",
    "\n",
    "One such loss function could be the **Outcome MSE loss**, which is the usual MSE loss function for prediction methods.\n",
    "$$\n",
    "\\mathcal{L}_{Y}(\\hat{\\mu}) = \\frac{1}{n} \\sum _ {i=1}^{n} \\Big( \\hat{\\mu}(X_i, W_i) - Y_i \\Big)^2\n",
    "$$\n",
    "\n",
    "The problem here is that not all models directly produce an estimate of $\\mu(x)$ and, even when they do, it is not the object of interest."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e90554ae-2a0b-47d4-b903-cd2ec7f050e9",
   "metadata": {},
   "source": [
    "### Prediction to Prediction Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f89d5f60-4d4f-464d-b2fc-8b26d0c738a5",
   "metadata": {},
   "source": [
    "Another very simple approach could be to compare the predictions of the model trained on the training set with the predictions of another model trained on the validation set. While intuitive, this appraoch could be **extremely misleading**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1955777-eab8-4833-b3c8-055a7cbe68dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_pred(data, learner):\n",
    "    tau = learner.effect(data[X])\n",
    "    learner2 = copy.deepcopy(learner).fit(data[Y], data[W], X=data[X])\n",
    "    tau2 = learner2.effect(data[X])\n",
    "    return np.mean((tau - tau2)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9964bd5a-2f02-490e-9aa2-e66b50cd408e",
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_methods(learners, names, loss_pred, 'Prediction to Prediction Loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89aaf6f9-b935-4c72-a29c-b79f1e6df954",
   "metadata": {},
   "source": [
    "Unsurprisingly, this metric performs extremely bad, and you should **never use it**, since it rewards models that are consistent, irrespectively of their quality. A model that always predicts a random constant CATE for each observations would obtain a perfect score."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e4d92c9-97b3-4fa3-bfc7-4aeebd63870a",
   "metadata": {},
   "source": [
    "### Distribution Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b632ae85-2c15-4930-acb3-9a535db0f73a",
   "metadata": {},
   "source": [
    "A different approach is to ask: how well can we match the distribution of potential outcomes? We can do this exarcise for either the *treated* or *untreated* potential outcomes. Let's take the last case. Suppose we take the observed `sales` for customers that did *not* receive the `mail` and the observed `sales` *minus* the estimated CATE $\\hat{\\tau}(x)$ for customers that did receive the `mail`. By the **unconfoundedness** assumption, these two distributions of the untreated potential outcome should be similar, conditional on covariates $X$.\n",
    "\n",
    "Therefore, we expect the distance between the two distributions to be close if we correctly estimated the treatment effects.\n",
    "$$\n",
    "dist \\ \\Big( \\ \\{Y_i, X_i | W_i=0 \\} \\ , \\ \\{Y_i - \\hat{\\tau}(X_i), X_i | W_i=1 \\} \\ \\Big)\n",
    "$$\n",
    "\n",
    "We can also do the same exercise for the *treated* potential outcome. \n",
    "\n",
    "$$\n",
    "dist \\ \\Big( \\ \\{Y_i + \\hat{\\tau}(X_i), X_i | W_i=0 \\} \\ , \\ \\{Y_i, X_i | W_i=1 \\} \\ \\Big)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e59a7d0-59e4-4b55-be65-5a9e0abf8622",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from dcor import energy_distance\n",
    "\n",
    "def loss_dist(data, learner):\n",
    "    tau = learner.effect(data[X])\n",
    "    data.loc[data.mail==1, 'sales'] -= tau[data.mail==1]\n",
    "    return energy_distance(data.loc[data.mail==0, [Y] + X], data.loc[data.mail==1, [Y] + X], exponent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9fc4302-1091-4572-aa66-a6ade0881295",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "compare_methods(learners, names, loss_dist, 'Distribution Loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f80b13b9-7283-40e2-91c7-d76b68c6b66a",
   "metadata": {},
   "source": [
    "This measure is extremely noisy and rewards the S-learner followed by the T-learner which are actually the two worst performing models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9666adba-b5d7-43c6-ac0b-8d31ea14eb66",
   "metadata": {},
   "source": [
    "### Above-below Median Difference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "534f65a2-dc9e-445b-918d-8be3ba94ccf9",
   "metadata": {},
   "source": [
    "The above-below median loss tries to answer the question: is our uplift model detecting **any heterogeneity**? In particular, if we take the validation set and we split the sample into above-median and below median predicted uplift $\\hat{\\tau}(x)$, how big is the actual difference in average effect, estimated with a difference-in-means estimator? We would expect better estimators to better split the sample into high-effects and low-effects. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c011cea-cba7-44da-a2f1-29dd5d8769cc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from statsmodels.formula.api import ols \n",
    "\n",
    "def loss_ab(data, learner):\n",
    "    tau = learner.effect(data[X]) + np.random.normal(0, 1e-8, len(data))\n",
    "    data['above_median'] = tau >= np.median(tau)\n",
    "    param = ols('sales ~ mail * above_median', data=data).fit().params[-1]\n",
    "    return param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f42a5c-4882-411a-be55-139c1c954264",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "compare_methods(learners, names, loss_ab, title='Above-below Median Difference', subtitle='higher is better')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49df916a-e1fd-4c75-86cb-2fc8d026ab18",
   "metadata": {},
   "source": [
    "Unfortunately, the above-below median difference rewards the T-learner, which is among the worst performing models. \n",
    "\n",
    "It's important to note that the difference-in-means estimators in the two groups (above- and below- median $\\hat{\\tau}(x)$) are **not guaranteed to be unbiased**, even if the data came from a randomized experiment. In fact, we have split the two groups on a variable, $\\hat{\\tau}(x)$, that is highly endogenous. Therefore, the method should be used with a grain of salt."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffa57206-b558-4b55-8fa3-6de113207b5f",
   "metadata": {},
   "source": [
    "### Uplift Curve"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03f63f7f-f1ba-4d5d-b038-871432113e7b",
   "metadata": {},
   "source": [
    "An extension of the above-below median test is the **uplift curve**. The idea is simple: instead of splitting the sample into two groups based on the median (0.5 quantile), why not split the data into more groups (more quantiles)?\n",
    "\n",
    "For each group, we compute the difference-in-means estimate, and we plot its cumulative sum against the corresponding quantile. The result is called **uplift curve**. The interpretation is simple: the higher the curve, the better we are able to separate high- from low-effect observations. However, also the same **disclaimer** applies: the difference-in-means estimates are not unbiased. Therefore, they should be used with a grain of salt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0feb17b-157d-472e-b057-1f43b2e7e1ef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_uplift_curve(df):\n",
    "    Q = 20\n",
    "    df_q = pd.DataFrame()\n",
    "    data = dgp.generate_data(seed_data=1, seed_assignment=1, keep_po=True)\n",
    "    ate = np.mean(data[Y][data[W]==1]) - np.mean(data[Y][data[W]==0])\n",
    "    for learner, name in zip(learners, names):\n",
    "        data['tau_hat'] = learner.effect(data[X])\n",
    "        data['q'] = pd.qcut(-data.tau_hat + np.random.normal(0, 1e-8, len(data)), q=Q, labels=False)\n",
    "        for q in range(Q):\n",
    "            temp = data[data.q <= q]\n",
    "            uplift = (np.mean(temp[Y][temp[W]==1]) - np.mean(temp[Y][temp[W]==0])) * q / (Q-1)\n",
    "            df_q = pd.concat([df_q, pd.DataFrame({'q': [q], 'uplift': [uplift], 'learner': [name]})], ignore_index=True)\n",
    "    \n",
    "    fig, ax = plt.subplots(1, 1, figsize=(8, 5))\n",
    "    sns.lineplot(x=range(Q), y=ate*range(Q)/(Q-1), color='k', ls='--', lw=3)\n",
    "    sns.lineplot(x='q', y='uplift', hue='learner', data=df_q);\n",
    "    plt.suptitle('Uplift Curve', y=1.02, fontsize=28, fontweight='bold')\n",
    "    plt.title('higher is better', fontsize=14, fontweight=None, y=0.96)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de2b3495-82db-43df-b058-6dcc17225756",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_uplift_curve(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2c4d9d7-b5c6-45aa-b000-406fbd7c1eb2",
   "metadata": {},
   "source": [
    "While probably not the best method to *evaluate* uplift models, the uplift curve is very important in **understanding** and **implementing** them. In fact, for each model, it tells us that is the expected average treatment effect (y-axis) as we increase the share of the treated population (x-axis)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6ebb17d-a466-4e71-b7c1-dfb06bf1cb04",
   "metadata": {},
   "source": [
    "### Nearest Neighbor Match"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e85bec-947f-436f-91c1-955c8246a0f3",
   "metadata": {},
   "source": [
    "The last couple of methods we analyzed, aggregated data in order to understand whether the methods work on larger groups. The nearest neighbor match tries instead to understand how well an uplift model predicts individual treatment effects. However, since the ITEs are not observable, it tries to build a **proxy by matching** treated and control observations on observable characteristics $X$.\n",
    "\n",
    "For example, if we take all treated observations ($i: W_i=1$), and we find the nearest neighbor in the control group ($NN_0(X_i)$), the corresponding MSE loss function is\n",
    "$$\n",
    "\\mathcal{L} _ {NN}(\\hat{\\tau}) = \\frac{1}{n} \\sum _ {i: W_i=1} \\Big( \\hat{\\tau}(X_i) - (Y_i - NN_0(X_i)) \\Big)^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d03d3cf2-afc1-4ff6-b216-31a2f0839c80",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from scipy.spatial import KDTree\n",
    "\n",
    "def loss_nn(data, learner):\n",
    "    tau_hat = learner.effect(data[X])\n",
    "    nn0 = KDTree(data.loc[data[W]==0, X].values)\n",
    "    control_index = nn0.query(data.loc[data[W]==1, X], k=1)[-1]\n",
    "    tau_nn = data.loc[data[W]==1, Y].values - data.iloc[control_index, :][Y].values\n",
    "    return np.mean((tau_hat[data[W]==1] - tau_nn)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35bf93bc-4fce-452c-89ad-f8b4615ac927",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "compare_methods(learners, names, loss_nn, title='Nearest Neighbor Loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b8e774b-c643-4be8-b353-6b0084f3d014",
   "metadata": {},
   "source": [
    "In this case, the nearest neighbor loss performs quite well, identifying the two worse performing methods, the S- and T-learner."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bddf7dc0-2d27-41ac-83e8-89de98c4a84c",
   "metadata": {},
   "source": [
    "### IPW Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67cd01ff-ccda-4a8b-93b2-20ea720427ff",
   "metadata": {},
   "source": [
    "The Inverse Probability Weighting (IPW) loss function was first proposed by [Gutierrez, Gerardy (2017)](https://proceedings.mlr.press/v67/gutierrez17a/gutierrez17a.pdf), and it is the first of three metrics that we are going to see that uses a **pseudo-outcome** $Y^{\\*}$ to evaluate the estimator. Pseudo-outcomes are variables whose expected value is the Conditional Average Treatment Effect, but that are too volatile to be directly used as estimates. For a more detailed explanation of pseudo-outcomes, I suggest [my article on causal regression trees](https://towardsdatascience.com/920177462149). The pseudo-outcome corresponding to the IPW loss is\n",
    "$$\n",
    "Y^* _ {IPW} = Y_i \\frac{W_i - \\hat{e}(X_i)}{\\hat{e}(X_i)(1 - \\hat{e}(X_i))}\n",
    "$$\n",
    "\n",
    "so that the corresponding loss function is\n",
    "$$\n",
    "\\mathcal{L} _ {IPW} = \\frac{1}{n} \\sum_{i=1}^{n} \\left( \\hat{\\tau}(X_i) - Y_i \\ \\frac{W_i - \\hat{e}(X_i)}{\\hat{e}(X_i)(1 - \\hat{e}(X_i))} \\right)^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba20f329-a881-4e53-a88e-305f78a099b8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def loss_ipw(data, learner):\n",
    "    tau_hat = learner.effect(data[X])\n",
    "    e_hat = clone(model_e).fit(data[X], data[W]).predict_proba(data[X])[:,1]\n",
    "    tau_gg = data[Y] * (data[W] - e_hat) / (e_hat * (1 - e_hat))\n",
    "    return np.mean((tau_hat - tau_gg)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afad1bb8-6f91-485c-a044-60323f5ee993",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "compare_methods(learners, names, loss_ipw, title='IPW Loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d28ff67-1a02-42a1-b9c4-3dfc6aa7659f",
   "metadata": {},
   "source": [
    "The IPW loss is extremely noisy. A solution is to use its more robust variations, the R-loss or the DR-loss which we present next."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1847010a-d754-47f3-92f3-0c83a2d02f30",
   "metadata": {},
   "source": [
    "### R Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e068043b-f6db-4f78-85af-020aa0661ae4",
   "metadata": {},
   "source": [
    "The R-loss was introduced together with the R-learner by [Nie, Wager (2017)](https://arxiv.org/abs/1712.04912), and it is essentially the **objective function** of the R-learner. As for the IPW-loss, the idea is to try to match a pseudo outcome whose expected value is the Conditional Average Treatment Effect.\n",
    "$$\n",
    "Y^* _ {R} = \\frac{Y_i - \\hat{\\mu}_W(X_i)}{W_i - \\hat{e}(X_i)}\n",
    "$$\n",
    "\n",
    "The corresponding loss function is\n",
    "$$\n",
    "\\mathcal{L}_{R} = \\frac{1}{n} \\sum _ {i=1}^{n} \\left( \\hat{\\tau}(X_i) -  \\frac{Y_i - \\hat{\\mu}_W(X_i)}{W_i - \\hat{e}(X_i)} \\right)^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "306ca4a0-7582-419b-b4f8-0645a0194232",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def loss_r(data, learner):\n",
    "    tau_hat = learner.effect(data[X])\n",
    "    y_hat = clone(model_y).fit(df[X + [W]], df[Y]).predict(data[X + [W]])\n",
    "    e_hat = clone(model_e).fit(df[X], df[W]).predict_proba(data[X])[:,1]\n",
    "    tau_nw = (data[Y] - y_hat) / (data[W] - e_hat)\n",
    "    return np.mean((tau_hat - tau_nw)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dead700-3229-4508-b03d-64d4b4b5e177",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "results = compare_methods(learners, names, loss_r, title='R Loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80ee9fed-c3c9-4e32-bf1e-70c9a8a4b762",
   "metadata": {},
   "source": [
    "The R-loss is sensibly less noisy than the IPW loss and it clearly isolates the S-learner. However, it tends to favor its corresponding learner, the R-learner."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72be227d-19e8-45d9-888f-efcc49e40ae0",
   "metadata": {},
   "source": [
    "### DR Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1cd0530-9e39-4d13-bd41-8b58f559cf03",
   "metadata": {},
   "source": [
    "The DR-loss is the **objective function** of the DR-learner, and it was first introduced by [Saito, Yasui (2020)](https://arxiv.org/abs/1909.05299). As for the IPW- and the R-loss, the idea is to try to match a pseudo outcome, whose expected value is the Conditional Average Treatment Effect. The DR pseudo-outcome is strongly related to the [AIPW estimator](https://towardsdatascience.com/ed4097dab27a), also known as doubly-robust estimator, hence the DR name.\n",
    "$$\n",
    "Y^* _ {DR} = \\hat{\\mu}_1(X_i) - \\hat{\\mu}_0(X_i) + (Y_i - \\hat{\\mu}_W(X_i)) \\ \\frac{W_i - \\hat{e}(X_i)}{\\hat{e}(X_i)(1 - \\hat{e}(X_i))}\n",
    "$$\n",
    "\n",
    "The corresponding loss function is\n",
    "$$\n",
    "\\mathcal{L} _ {DR} = \\frac{1}{n} \\sum _ {i=1}^{n} \\left( \\hat{\\tau}(X_i) - \\hat{\\mu}_1(X_i) + \\hat{\\mu}_0(X_i) - (Y_i - \\hat{\\mu}_W(X_i)) \\ \\frac{W_i - \\hat{e}(X_i)}{\\hat{e}(X_i)(1 - \\hat{e}(X_i))} \\right)^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8293269-6b97-49af-8a88-d18dcaeec8c5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def loss_dr(data, learner):\n",
    "    tau_hat = learner.effect(data[X])\n",
    "    y_hat = clone(model_y).fit(df[X + [W]], df[Y]).predict(data[X + [W]])\n",
    "    mu1 = clone(model_y).fit(df[X + [W]], df[Y]).predict(data[X + [W]].assign(mail=1))\n",
    "    mu0 = clone(model_y).fit(df[X + [W]], df[Y]).predict(data[X + [W]].assign(mail=0))\n",
    "    e_hat = clone(model_e).fit(df[X], df[W]).predict_proba(data[X])[:,1]\n",
    "    tau_nw = mu1 - mu0 + (data[Y] - y_hat) * (data[W] - e_hat) / (e_hat * (1 - e_hat))\n",
    "    return np.mean((tau_hat - tau_nw)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf501a12-0ccf-4cb1-8ba9-2de8f6949fd8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "results = compare_methods(learners, names, loss_dr, title='DR Loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e160d9c4-a86c-4964-9e66-45db783a0c71",
   "metadata": {},
   "source": [
    "As for the R-loss, the DR-loss tends to favor its corresponding learner, the DR-learner. However, it provides a more accurate ranking in terms of algorithms' accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e08ccd1c-3c47-4256-b917-723d3dcaa356",
   "metadata": {},
   "source": [
    "### Empirical Policy Gain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "854ae519-119f-4dfe-a5eb-cca8b58f48f5",
   "metadata": {},
   "source": [
    "The last loss function that we are going to analyze is different from all the others we have seen so far since it does *not* focus on how well we are able to estimate the treatment effects but rather on how well would the corresponding **optimal treatment policy** performs. In particular, [Hitsch, Misra, Zhang (2023)](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3111957) propose the following gain function:\n",
    "\n",
    "$$\n",
    "\\mathcal{G} _ {HMZ} = \\sum _ {i=1}^{n} \\left( W_i \\cdot d(\\hat{\\tau}) \\cdot \\frac{Y_i - c}{\\hat{e}(X_i)} + (1-W_i) \\cdot (1-d(\\hat{\\tau})) \\cdot \\frac{Y_i}{1-\\hat{e}(X_i)} \\right)\n",
    "$$\n",
    "\n",
    "where $c$ is the treatment cost and $d$ is the optimal treatment policy given the estimated CATE $\\hat{\\tau}(X_i)$. In our case, we assume an individual treatment cost of $c=0.01$\\\\$, so that the optimal policy is to treat every customer with an estimated CATE larger than 0.01.\n",
    "\n",
    "The terms $W_i \\cdot d(X_i)$ and $(1-W_i) \\cdot (1-d(X_i))$ imply that we use for the calculation only individuals for whom the actual treatment *W* corresponds with the optimal one, *d*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe039f5-ee0c-4514-9347-742f99981d3e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def gain_policy(data, learner):\n",
    "    tau_hat = learner.effect(data[X])\n",
    "    e_hat = clone(model_e).fit(data[X], data[W]).predict_proba(data[X])[:,1]\n",
    "    d = tau_hat > cost\n",
    "    return np.sum((d * data[W] * (data[Y] - cost)/ e_hat + (1-d) * (1-data[W]) * data[Y] / (1-e_hat)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "490f2184-f671-47cf-90f0-9158ed94d5b6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "results = compare_methods(learners, names, gain_policy, title='Empirical Policy Gain', subtitle='higher is better')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8faf9856-74fe-4166-bc8e-c9bbae33388e",
   "metadata": {},
   "source": [
    "The empirical policy gain performs very well, isolating the two worst performing methods, the S- and T-learners."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f2680a-7aa4-42bd-8a70-1618c507b77b",
   "metadata": {},
   "source": [
    "## Meta Studies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef7cd3de-569c-4471-9b43-471bff2751f1",
   "metadata": {},
   "source": [
    "In this article we have introduced a wide variety of methods to evaluate uplift models, a.k.a. Conditional Average Treatment Effect estimators. We have also tested in our simulated dataset, which is a very special and limited example. How do these metrics **perform** in general? \n",
    "\n",
    "[Schuler, Baiocchi, Tibshirani, Shah (2018)](https://arxiv.org/abs/1804.05146) compares the S-loss, T-loss, R-loss, on **simulated data**, for the corresponding estimators. They find that the R-loss \"*is the validation set metric that, when optimized, most consistently leads to the selection of a high-performing model*\". The authors also detect the so-called **congeniality bias**: metrics such as the R- or DR-loss tend to be biased towards the corresponding learner.\n",
    "\n",
    "[Curth, van der Schaar (2023)](https://arxiv.org/abs/2302.02923) studies a broader array of learners from a **theoretical perspective**. They find that \"*no existing selection criterion is globally best across all experimental conditions we consider*\". \n",
    "\n",
    "[Mahajan, Mitliagkas, Neal, Syrgkanis (2023)](https://arxiv.org/abs/2211.01939) is the **most comprehensive** study in terms of scope. The authors compare many metrics on 144 datasets and 415 estimators. They find that “*no metric significantly dominates the rest*” but “*metrics that use DR elements seem to always be among the candidate winners*”."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c79cf7e-d9a7-4174-948c-cd3a7fd781ef",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc944d68-3b3d-44ff-be91-3e79e1bb69b6",
   "metadata": {},
   "source": [
    "In this article, we have explored multiple methods to evaluate uplift models. The **main challenge** is the unobservability of the variable of interest, the Individual Treatment Effects. Therefore, different methods try to evaluate uplift models either using other variables, using proxy outcomes, or approximating the effect of implied optimal policies.\n",
    "\n",
    "It is hard to recommend using a single method since there is **no consensus** on which one performs best, neither from a theoretical nor from an empirical perspective. Loss functions that use R- and DR- elements tend to perform **consistently better**, but are also biased towards the corresponding learners. Understanding how these metrics work, however, can help in understanding their biases and limitations in order to make the most appropriate decisions depending on the specific scenario."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b19124c-5511-4862-913e-2084fda06c7b",
   "metadata": {
    "tags": []
   },
   "source": [
    "### References"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1227d84-b217-4120-850b-89a1004be77f",
   "metadata": {},
   "source": [
    "- Curth, van der Schaar (2023), [\"In Search of Insights, Not Magic Bullets: Towards Demystification of the Model Selection Dilemma in Heterogeneous Treatment Effect Estimation\"](https://arxiv.org/abs/2302.02923)\n",
    "\n",
    "- Gutierrez, Gerardy (2017), [\"Causal Inference and Uplift Modeling: A review of the literature\"](https://proceedings.mlr.press/v67/gutierrez17a/gutierrez17a.pdf)\n",
    "\n",
    "- Hitsch, Misra, Zhang (2023), [\"Heterogeneous Treatment Effects and Optimal Targeting Policy Evaluation\"](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3111957)\n",
    "\n",
    "- Kennedy (2022), [\"Towards optimal doubly robust estimation of heterogeneous causal effects\"](https://arxiv.org/abs/2004.14497)\n",
    "\n",
    "- Kunzel, Sekhon, Bickel, Yu (2017), [\"Meta-learners for Estimating Heterogeneous Treatment Effects using Machine Learning\"](https://arxiv.org/abs/1706.03461)\n",
    "\n",
    "- Mahajan, Mitliagkas, Neal, Syrgkanis (2023), [\"Empirical Analysis of Model Selection for Heterogeneous Causal Effect Estimation\"](https://arxiv.org/abs/2211.01939)\n",
    "\n",
    "- Nie, Wager (2017), [\"Quasi-Oracle Estimation of Heterogeneous Treatment Effects\"](https://arxiv.org/abs/1712.04912)\n",
    "\n",
    "- Saito, Yasui (2020), [\"Counterfactual Cross-Validation: Stable Model Selection Procedure for Causal Inference Models\"](https://arxiv.org/abs/1909.05299)\n",
    "\n",
    "- Schuler, Baiocchi, Tibshirani, Shah (2018), [\"A comparison of methods for model selection when estimating individual treatment effects\"](https://arxiv.org/abs/1804.05146)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e208773-1860-4653-b1eb-26feaf6a6f86",
   "metadata": {},
   "source": [
    "### Related Articles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf2d42b-193c-40f0-899d-15678d3281a6",
   "metadata": {},
   "source": [
    "- [Understanding Meta Learners](https://towardsdatascience.com/8a9c1e340832)\n",
    "\n",
    "- [Understanding AIPW, the Doubly-Robust Estimator](https://towardsdatascience.com/ed4097dab27a)\n",
    "\n",
    "- [Understanding Causal Trees](https://towardsdatascience.com/920177462149)\n",
    "\n",
    "- [From Causal Trees to Forests](https://towardsdatascience.com/43c4536f1481)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9bd4b92-d4ef-4eeb-988d-e42257f8b6e3",
   "metadata": {},
   "source": [
    "### Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fcc472e-db7e-45b5-bca8-8b4e869eee21",
   "metadata": {},
   "source": [
    "You can find the original Jupyter Notebook here:\n",
    "\n",
    "https://github.com/matteocourthoud/Blog-Posts/blob/main/notebooks/evaluate_uplift.ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
